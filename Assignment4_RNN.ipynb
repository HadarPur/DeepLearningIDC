{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 4: RNN",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2Acse04FKm9"
      },
      "source": [
        "\n",
        "# Assignment 4 \n",
        "In this assignment you will be working with a character based LSTM language model, which you will turn into a text classifier for sentiment analysis using **Attention**. For that, you will need to develop the Attention mechanism that aggregates the hidden output vectors that you get per character into a single vector, which you will use as an input for a final linear classifier.\n",
        "\n",
        "\n",
        "Submitted:\n",
        "\n",
        "*   Hadar Pur\n",
        "*   Rotem Feinblat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2_0Ukwaw58u"
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from random import sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4wNWnaQ9x8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a0c2a1b-fbbc-46d7-8c08-94a257aaf74d"
      },
      "source": [
        "!git clone https://github.com/kfirbar/course-ml-data.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'course-ml-data'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 31 (delta 5), reused 8 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (31/31), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLiw2Xn4Fp2g"
      },
      "source": [
        "The data that you will be working with is SST-2, which is a collection of reviews, each is classified into 0/1 reflecting the overall sentiment of the author (0 = negative, 1 = positive). In the next piece of code, we load the data and create a dictionary (named Vocab) that assigns a unique ID per character, similar to what have done in DL Notebook 12. Finally, each one of *train* and *dev* is a list of tuples, with the first item being the text encoded as character indices, and the second item is the label (0, 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQnd3bS1xJxN",
        "outputId": "4093c4d7-af47-4eb7-a633-d8a48ab0fb23"
      },
      "source": [
        "# We will work only with texts of size < 50 characters\n",
        "MAX_SEQ_LEN = 50\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self):\n",
        "        self.char2id = {}\n",
        "        self.id2char = {}\n",
        "        self.n_chars = 1\n",
        "        \n",
        "    def index_text(self, text):\n",
        "      indexes = [self.index_char(c) for c in text]\n",
        "      return indexes\n",
        "    \n",
        "    def index_char(self, c):\n",
        "        if c not in self.char2id:\n",
        "            self.char2id[c] = self.n_chars\n",
        "            self.id2char[self.n_chars] = c\n",
        "            self.n_chars += 1\n",
        "        return self.char2id[c]\n",
        "            \n",
        "            \n",
        "def load_data(data, vocab):\n",
        "  data_sequences = []\n",
        "  for text in data.iterrows():\n",
        "    if len(text[1][\"sentence\"]) <= MAX_SEQ_LEN:\n",
        "      indexes = vocab.index_text(text[1][\"sentence\"])\n",
        "      data_sequences.append((indexes, text[1][\"label\"]))\n",
        "  return data_sequences\n",
        "\n",
        "vocab = Vocab()\n",
        "train = load_data(pd.read_csv('/content/course-ml-data/SST2_train.tsv', sep='\\t'), vocab)\n",
        "dev = load_data(pd.read_csv('/content/course-ml-data/SST2_dev.tsv', sep='\\t'), vocab)\n",
        "print(f'Train size {len(train)}, Dev size {len(dev)}, vocab size {vocab.n_chars}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size 40625, Dev size 119, vocab size 63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaeBB5qgLgGF"
      },
      "source": [
        "# Task 1\n",
        "The following RNN architectures takes a single sentence as an input (formatted as a 1D tensor of character ids), and returns a distribution over the labels. In our case the number of labels is 2 (negative, positive). \n",
        "\n",
        "I basically copied the same architecture from Notebook 12, where each input character gets an output vector from the LSTM module, which are used to precdict the next character in line. However, here, we are not really interested in predicting the next character, but in aggregating all those output vectors into a single \"context\" vector, which will be sent to a Linear layer for the final classification step.\n",
        "\n",
        "Therefore, you are requested to add the relevant code for aggregating the output vectors using the **additive attention** approach, following presentation *DL 14*. Note that some of what you need to add should be parameters, which you need to define under the __init__ function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIekWqzPxMOL"
      },
      "source": [
        "#@title\n",
        "class SeqModel(torch.nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, output_size):\n",
        "    super(SeqModel, self).__init__()\n",
        "    ##input size , hidden size - the size of the embbeding that we create \n",
        "    self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
        "    ##the embedding create the weigths \n",
        "    ##lstm create the vector h\n",
        "    self.rnn = torch.nn.LSTM(embedding_size, hidden_size)\n",
        "    \n",
        "    # TODO: add the relevant initialization code for the Attention mechanism\n",
        "    ##need to define (say, new_linear in our case)\n",
        "    self.new_linear=torch.nn.Linear(hidden_size, hidden_size)\n",
        "    ##new trainable v vector,We define v as a tensor of the size hidden_size.a \n",
        "    self.v = torch.nn.Parameter(torch.randn(hidden_size)) \n",
        "    self.out = torch.nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, single_sentence):\n",
        "    # single_sentence is a 1D tensor containing indices of the sentence characters\n",
        "    embedded = self.embedding(single_sentence)\n",
        "    embedded = embedded.view(len(single_sentence), 1, -1) ##like reshape create tensor with 3 things. the first is the size of the sentence ,1 the second is the batch, \n",
        "    out, hidden = self.rnn(embedded) ## the out is h, and the hidden is h and c\n",
        "    # TODO: calculate the context vector, which is a weighted average of the out vectors, with weights learned automatically\n",
        "    out = out.squeeze()\n",
        "    o = torch.tanh(self.new_linear(out))\n",
        "\n",
        "    alpha = torch.nn.functional.softmax(torch.matmul(o, self.v)) \n",
        "    #Once we have the probabilities, need to multiply each output vector by its weight, and then sum them up element wise, to get a single vector.\n",
        "    context = torch.matmul(alpha, out)\n",
        "    return self.out(context).view(1, -1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUqEY4PZM6tM"
      },
      "source": [
        "# Task 2\n",
        "Once completed, you are now requested to write some code for training the model using the following configuration. Make sure to print the training loss every 100 sentences so you can follow. Train your code for 4 epochs, and use cuda + GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyRFPSOxND2h"
      },
      "source": [
        "model = SeqModel(vocab.n_chars, 64, 300, 2).cuda()\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDjUxq3u4J4S"
      },
      "source": [
        "def train_loss_calculation(epochs,batch_size):\r\n",
        "  train_loss_array=[]\r\n",
        "  test_loss_array=[]\r\n",
        "\r\n",
        "  # training loop\r\n",
        "  for e in range(epochs):  \r\n",
        "    running_loss = 0.0\r\n",
        "    train_loss = 0.0\r\n",
        "    for i, data in enumerate(train, 0):\r\n",
        "      # get the inputs\r\n",
        "      inputs, labels = data # -- For CPU\r\n",
        "        \r\n",
        "      inputs = torch.LongTensor(inputs).cuda() # -- For GPU\r\n",
        "      labels = torch.LongTensor([labels]).cuda() # -- For GPU\r\n",
        "\r\n",
        "      # zero the parameter gradients\r\n",
        "      optimizer.zero_grad()\r\n",
        "\r\n",
        "      # forward + backward + optimize\r\n",
        "      outputs = model(inputs)\r\n",
        "\r\n",
        "      loss = criterion(outputs, labels)\r\n",
        "      loss.backward()\r\n",
        "      optimizer.step()\r\n",
        "\r\n",
        "      # print statistics\r\n",
        "      running_loss += loss.item()\r\n",
        "      train_loss += loss.item()\r\n",
        "      if (i+1) % batch_size == 0:    \r\n",
        "        print('[%d, %5d/%d] Loss: %.3f' %(e + 1, i + 1,len(train), running_loss / batch_size))\r\n",
        "        running_loss = 0.0\r\n",
        "\r\n",
        "    # calculate train loss for epoch\r\n",
        "    train_loss_calc = train_loss / len(train)\r\n",
        "    train_loss_array.append(train_loss_calc)\r\n",
        "\r\n",
        "  return train_loss_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJMQCYFPNE5Z",
        "outputId": "e81b6735-cdf5-451e-d7e4-9a4db1c18d88"
      },
      "source": [
        "# setup\r\n",
        "batch_size = 100\r\n",
        "epochs = 4\r\n",
        "epoch_array = list(range(1,epochs+1))\r\n",
        "\r\n",
        "# train \r\n",
        "train_loss_array= train_loss_calculation(epochs, batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,   100/40625] Loss: 0.725\n",
            "[1,   200/40625] Loss: 0.724\n",
            "[1,   300/40625] Loss: 0.705\n",
            "[1,   400/40625] Loss: 0.701\n",
            "[1,   500/40625] Loss: 0.690\n",
            "[1,   600/40625] Loss: 0.684\n",
            "[1,   700/40625] Loss: 0.693\n",
            "[1,   800/40625] Loss: 0.776\n",
            "[1,   900/40625] Loss: 0.726\n",
            "[1,  1000/40625] Loss: 0.665\n",
            "[1,  1100/40625] Loss: 0.728\n",
            "[1,  1200/40625] Loss: 0.711\n",
            "[1,  1300/40625] Loss: 0.709\n",
            "[1,  1400/40625] Loss: 0.715\n",
            "[1,  1500/40625] Loss: 0.710\n",
            "[1,  1600/40625] Loss: 0.706\n",
            "[1,  1700/40625] Loss: 0.687\n",
            "[1,  1800/40625] Loss: 0.719\n",
            "[1,  1900/40625] Loss: 0.682\n",
            "[1,  2000/40625] Loss: 0.686\n",
            "[1,  2100/40625] Loss: 0.706\n",
            "[1,  2200/40625] Loss: 0.728\n",
            "[1,  2300/40625] Loss: 0.711\n",
            "[1,  2400/40625] Loss: 0.712\n",
            "[1,  2500/40625] Loss: 0.676\n",
            "[1,  2600/40625] Loss: 0.720\n",
            "[1,  2700/40625] Loss: 0.679\n",
            "[1,  2800/40625] Loss: 0.677\n",
            "[1,  2900/40625] Loss: 0.679\n",
            "[1,  3000/40625] Loss: 0.654\n",
            "[1,  3100/40625] Loss: 0.744\n",
            "[1,  3200/40625] Loss: 0.714\n",
            "[1,  3300/40625] Loss: 0.700\n",
            "[1,  3400/40625] Loss: 0.698\n",
            "[1,  3500/40625] Loss: 0.704\n",
            "[1,  3600/40625] Loss: 0.646\n",
            "[1,  3700/40625] Loss: 0.713\n",
            "[1,  3800/40625] Loss: 0.683\n",
            "[1,  3900/40625] Loss: 0.692\n",
            "[1,  4000/40625] Loss: 0.691\n",
            "[1,  4100/40625] Loss: 0.682\n",
            "[1,  4200/40625] Loss: 0.675\n",
            "[1,  4300/40625] Loss: 0.644\n",
            "[1,  4400/40625] Loss: 0.703\n",
            "[1,  4500/40625] Loss: 0.676\n",
            "[1,  4600/40625] Loss: 0.685\n",
            "[1,  4700/40625] Loss: 0.674\n",
            "[1,  4800/40625] Loss: 0.633\n",
            "[1,  4900/40625] Loss: 0.708\n",
            "[1,  5000/40625] Loss: 0.710\n",
            "[1,  5100/40625] Loss: 0.743\n",
            "[1,  5200/40625] Loss: 0.743\n",
            "[1,  5300/40625] Loss: 0.715\n",
            "[1,  5400/40625] Loss: 0.704\n",
            "[1,  5500/40625] Loss: 0.688\n",
            "[1,  5600/40625] Loss: 0.677\n",
            "[1,  5700/40625] Loss: 0.685\n",
            "[1,  5800/40625] Loss: 0.704\n",
            "[1,  5900/40625] Loss: 0.728\n",
            "[1,  6000/40625] Loss: 0.712\n",
            "[1,  6100/40625] Loss: 0.713\n",
            "[1,  6200/40625] Loss: 0.718\n",
            "[1,  6300/40625] Loss: 0.705\n",
            "[1,  6400/40625] Loss: 0.678\n",
            "[1,  6500/40625] Loss: 0.686\n",
            "[1,  6600/40625] Loss: 0.653\n",
            "[1,  6700/40625] Loss: 0.671\n",
            "[1,  6800/40625] Loss: 0.660\n",
            "[1,  6900/40625] Loss: 0.703\n",
            "[1,  7000/40625] Loss: 0.669\n",
            "[1,  7100/40625] Loss: 0.672\n",
            "[1,  7200/40625] Loss: 0.698\n",
            "[1,  7300/40625] Loss: 0.692\n",
            "[1,  7400/40625] Loss: 0.686\n",
            "[1,  7500/40625] Loss: 0.695\n",
            "[1,  7600/40625] Loss: 0.690\n",
            "[1,  7700/40625] Loss: 0.706\n",
            "[1,  7800/40625] Loss: 0.701\n",
            "[1,  7900/40625] Loss: 0.667\n",
            "[1,  8000/40625] Loss: 0.680\n",
            "[1,  8100/40625] Loss: 0.679\n",
            "[1,  8200/40625] Loss: 0.699\n",
            "[1,  8300/40625] Loss: 0.667\n",
            "[1,  8400/40625] Loss: 0.666\n",
            "[1,  8500/40625] Loss: 0.701\n",
            "[1,  8600/40625] Loss: 0.701\n",
            "[1,  8700/40625] Loss: 0.702\n",
            "[1,  8800/40625] Loss: 0.651\n",
            "[1,  8900/40625] Loss: 0.712\n",
            "[1,  9000/40625] Loss: 0.705\n",
            "[1,  9100/40625] Loss: 0.661\n",
            "[1,  9200/40625] Loss: 0.678\n",
            "[1,  9300/40625] Loss: 0.644\n",
            "[1,  9400/40625] Loss: 0.563\n",
            "[1,  9500/40625] Loss: 0.749\n",
            "[1,  9600/40625] Loss: 0.659\n",
            "[1,  9700/40625] Loss: 0.695\n",
            "[1,  9800/40625] Loss: 0.708\n",
            "[1,  9900/40625] Loss: 0.694\n",
            "[1, 10000/40625] Loss: 0.703\n",
            "[1, 10100/40625] Loss: 0.711\n",
            "[1, 10200/40625] Loss: 0.675\n",
            "[1, 10300/40625] Loss: 0.694\n",
            "[1, 10400/40625] Loss: 0.662\n",
            "[1, 10500/40625] Loss: 0.706\n",
            "[1, 10600/40625] Loss: 0.677\n",
            "[1, 10700/40625] Loss: 0.714\n",
            "[1, 10800/40625] Loss: 0.697\n",
            "[1, 10900/40625] Loss: 0.692\n",
            "[1, 11000/40625] Loss: 0.683\n",
            "[1, 11100/40625] Loss: 0.690\n",
            "[1, 11200/40625] Loss: 0.698\n",
            "[1, 11300/40625] Loss: 0.711\n",
            "[1, 11400/40625] Loss: 0.663\n",
            "[1, 11500/40625] Loss: 0.690\n",
            "[1, 11600/40625] Loss: 0.666\n",
            "[1, 11700/40625] Loss: 0.661\n",
            "[1, 11800/40625] Loss: 0.709\n",
            "[1, 11900/40625] Loss: 0.679\n",
            "[1, 12000/40625] Loss: 0.693\n",
            "[1, 12100/40625] Loss: 0.659\n",
            "[1, 12200/40625] Loss: 0.691\n",
            "[1, 12300/40625] Loss: 0.678\n",
            "[1, 12400/40625] Loss: 0.660\n",
            "[1, 12500/40625] Loss: 0.634\n",
            "[1, 12600/40625] Loss: 0.677\n",
            "[1, 12700/40625] Loss: 0.607\n",
            "[1, 12800/40625] Loss: 0.693\n",
            "[1, 12900/40625] Loss: 0.702\n",
            "[1, 13000/40625] Loss: 0.627\n",
            "[1, 13100/40625] Loss: 0.710\n",
            "[1, 13200/40625] Loss: 0.699\n",
            "[1, 13300/40625] Loss: 0.687\n",
            "[1, 13400/40625] Loss: 0.694\n",
            "[1, 13500/40625] Loss: 0.667\n",
            "[1, 13600/40625] Loss: 0.725\n",
            "[1, 13700/40625] Loss: 0.652\n",
            "[1, 13800/40625] Loss: 0.680\n",
            "[1, 13900/40625] Loss: 0.655\n",
            "[1, 14000/40625] Loss: 0.678\n",
            "[1, 14100/40625] Loss: 0.679\n",
            "[1, 14200/40625] Loss: 0.681\n",
            "[1, 14300/40625] Loss: 0.651\n",
            "[1, 14400/40625] Loss: 0.697\n",
            "[1, 14500/40625] Loss: 0.662\n",
            "[1, 14600/40625] Loss: 0.691\n",
            "[1, 14700/40625] Loss: 0.644\n",
            "[1, 14800/40625] Loss: 0.717\n",
            "[1, 14900/40625] Loss: 0.632\n",
            "[1, 15000/40625] Loss: 0.679\n",
            "[1, 15100/40625] Loss: 0.669\n",
            "[1, 15200/40625] Loss: 0.645\n",
            "[1, 15300/40625] Loss: 0.678\n",
            "[1, 15400/40625] Loss: 0.613\n",
            "[1, 15500/40625] Loss: 0.611\n",
            "[1, 15600/40625] Loss: 0.678\n",
            "[1, 15700/40625] Loss: 0.681\n",
            "[1, 15800/40625] Loss: 0.692\n",
            "[1, 15900/40625] Loss: 0.647\n",
            "[1, 16000/40625] Loss: 0.655\n",
            "[1, 16100/40625] Loss: 0.661\n",
            "[1, 16200/40625] Loss: 0.636\n",
            "[1, 16300/40625] Loss: 0.630\n",
            "[1, 16400/40625] Loss: 0.596\n",
            "[1, 16500/40625] Loss: 0.714\n",
            "[1, 16600/40625] Loss: 0.677\n",
            "[1, 16700/40625] Loss: 0.657\n",
            "[1, 16800/40625] Loss: 0.646\n",
            "[1, 16900/40625] Loss: 0.662\n",
            "[1, 17000/40625] Loss: 0.637\n",
            "[1, 17100/40625] Loss: 0.688\n",
            "[1, 17200/40625] Loss: 0.652\n",
            "[1, 17300/40625] Loss: 0.646\n",
            "[1, 17400/40625] Loss: 0.668\n",
            "[1, 17500/40625] Loss: 0.705\n",
            "[1, 17600/40625] Loss: 0.664\n",
            "[1, 17700/40625] Loss: 0.610\n",
            "[1, 17800/40625] Loss: 0.605\n",
            "[1, 17900/40625] Loss: 0.637\n",
            "[1, 18000/40625] Loss: 0.663\n",
            "[1, 18100/40625] Loss: 0.619\n",
            "[1, 18200/40625] Loss: 0.639\n",
            "[1, 18300/40625] Loss: 0.662\n",
            "[1, 18400/40625] Loss: 0.698\n",
            "[1, 18500/40625] Loss: 0.686\n",
            "[1, 18600/40625] Loss: 0.648\n",
            "[1, 18700/40625] Loss: 0.664\n",
            "[1, 18800/40625] Loss: 0.624\n",
            "[1, 18900/40625] Loss: 0.618\n",
            "[1, 19000/40625] Loss: 0.666\n",
            "[1, 19100/40625] Loss: 0.667\n",
            "[1, 19200/40625] Loss: 0.627\n",
            "[1, 19300/40625] Loss: 0.630\n",
            "[1, 19400/40625] Loss: 0.686\n",
            "[1, 19500/40625] Loss: 0.631\n",
            "[1, 19600/40625] Loss: 0.567\n",
            "[1, 19700/40625] Loss: 0.649\n",
            "[1, 19800/40625] Loss: 0.678\n",
            "[1, 19900/40625] Loss: 0.605\n",
            "[1, 20000/40625] Loss: 0.639\n",
            "[1, 20100/40625] Loss: 0.602\n",
            "[1, 20200/40625] Loss: 0.677\n",
            "[1, 20300/40625] Loss: 0.627\n",
            "[1, 20400/40625] Loss: 0.651\n",
            "[1, 20500/40625] Loss: 0.551\n",
            "[1, 20600/40625] Loss: 0.679\n",
            "[1, 20700/40625] Loss: 0.647\n",
            "[1, 20800/40625] Loss: 0.638\n",
            "[1, 20900/40625] Loss: 0.687\n",
            "[1, 21000/40625] Loss: 0.691\n",
            "[1, 21100/40625] Loss: 0.606\n",
            "[1, 21200/40625] Loss: 0.631\n",
            "[1, 21300/40625] Loss: 0.632\n",
            "[1, 21400/40625] Loss: 0.603\n",
            "[1, 21500/40625] Loss: 0.632\n",
            "[1, 21600/40625] Loss: 0.666\n",
            "[1, 21700/40625] Loss: 0.622\n",
            "[1, 21800/40625] Loss: 0.571\n",
            "[1, 21900/40625] Loss: 0.560\n",
            "[1, 22000/40625] Loss: 0.642\n",
            "[1, 22100/40625] Loss: 0.561\n",
            "[1, 22200/40625] Loss: 0.685\n",
            "[1, 22300/40625] Loss: 0.581\n",
            "[1, 22400/40625] Loss: 0.666\n",
            "[1, 22500/40625] Loss: 0.639\n",
            "[1, 22600/40625] Loss: 0.665\n",
            "[1, 22700/40625] Loss: 0.576\n",
            "[1, 22800/40625] Loss: 0.663\n",
            "[1, 22900/40625] Loss: 0.593\n",
            "[1, 23000/40625] Loss: 0.651\n",
            "[1, 23100/40625] Loss: 0.585\n",
            "[1, 23200/40625] Loss: 0.602\n",
            "[1, 23300/40625] Loss: 0.603\n",
            "[1, 23400/40625] Loss: 0.645\n",
            "[1, 23500/40625] Loss: 0.642\n",
            "[1, 23600/40625] Loss: 0.661\n",
            "[1, 23700/40625] Loss: 0.639\n",
            "[1, 23800/40625] Loss: 0.622\n",
            "[1, 23900/40625] Loss: 0.616\n",
            "[1, 24000/40625] Loss: 0.605\n",
            "[1, 24100/40625] Loss: 0.599\n",
            "[1, 24200/40625] Loss: 0.568\n",
            "[1, 24300/40625] Loss: 0.643\n",
            "[1, 24400/40625] Loss: 0.591\n",
            "[1, 24500/40625] Loss: 0.623\n",
            "[1, 24600/40625] Loss: 0.659\n",
            "[1, 24700/40625] Loss: 0.628\n",
            "[1, 24800/40625] Loss: 0.561\n",
            "[1, 24900/40625] Loss: 0.614\n",
            "[1, 25000/40625] Loss: 0.617\n",
            "[1, 25100/40625] Loss: 0.563\n",
            "[1, 25200/40625] Loss: 0.610\n",
            "[1, 25300/40625] Loss: 0.584\n",
            "[1, 25400/40625] Loss: 0.658\n",
            "[1, 25500/40625] Loss: 0.612\n",
            "[1, 25600/40625] Loss: 0.657\n",
            "[1, 25700/40625] Loss: 0.636\n",
            "[1, 25800/40625] Loss: 0.643\n",
            "[1, 25900/40625] Loss: 0.591\n",
            "[1, 26000/40625] Loss: 0.588\n",
            "[1, 26100/40625] Loss: 0.606\n",
            "[1, 26200/40625] Loss: 0.603\n",
            "[1, 26300/40625] Loss: 0.537\n",
            "[1, 26400/40625] Loss: 0.581\n",
            "[1, 26500/40625] Loss: 0.654\n",
            "[1, 26600/40625] Loss: 0.573\n",
            "[1, 26700/40625] Loss: 0.574\n",
            "[1, 26800/40625] Loss: 0.635\n",
            "[1, 26900/40625] Loss: 0.575\n",
            "[1, 27000/40625] Loss: 0.650\n",
            "[1, 27100/40625] Loss: 0.592\n",
            "[1, 27200/40625] Loss: 0.571\n",
            "[1, 27300/40625] Loss: 0.655\n",
            "[1, 27400/40625] Loss: 0.558\n",
            "[1, 27500/40625] Loss: 0.641\n",
            "[1, 27600/40625] Loss: 0.633\n",
            "[1, 27700/40625] Loss: 0.534\n",
            "[1, 27800/40625] Loss: 0.549\n",
            "[1, 27900/40625] Loss: 0.540\n",
            "[1, 28000/40625] Loss: 0.615\n",
            "[1, 28100/40625] Loss: 0.570\n",
            "[1, 28200/40625] Loss: 0.570\n",
            "[1, 28300/40625] Loss: 0.625\n",
            "[1, 28400/40625] Loss: 0.569\n",
            "[1, 28500/40625] Loss: 0.605\n",
            "[1, 28600/40625] Loss: 0.566\n",
            "[1, 28700/40625] Loss: 0.554\n",
            "[1, 28800/40625] Loss: 0.641\n",
            "[1, 28900/40625] Loss: 0.586\n",
            "[1, 29000/40625] Loss: 0.512\n",
            "[1, 29100/40625] Loss: 0.551\n",
            "[1, 29200/40625] Loss: 0.585\n",
            "[1, 29300/40625] Loss: 0.632\n",
            "[1, 29400/40625] Loss: 0.586\n",
            "[1, 29500/40625] Loss: 0.564\n",
            "[1, 29600/40625] Loss: 0.525\n",
            "[1, 29700/40625] Loss: 0.640\n",
            "[1, 29800/40625] Loss: 0.579\n",
            "[1, 29900/40625] Loss: 0.554\n",
            "[1, 30000/40625] Loss: 0.603\n",
            "[1, 30100/40625] Loss: 0.518\n",
            "[1, 30200/40625] Loss: 0.516\n",
            "[1, 30300/40625] Loss: 0.639\n",
            "[1, 30400/40625] Loss: 0.527\n",
            "[1, 30500/40625] Loss: 0.539\n",
            "[1, 30600/40625] Loss: 0.541\n",
            "[1, 30700/40625] Loss: 0.471\n",
            "[1, 30800/40625] Loss: 0.658\n",
            "[1, 30900/40625] Loss: 0.566\n",
            "[1, 31000/40625] Loss: 0.570\n",
            "[1, 31100/40625] Loss: 0.542\n",
            "[1, 31200/40625] Loss: 0.582\n",
            "[1, 31300/40625] Loss: 0.521\n",
            "[1, 31400/40625] Loss: 0.549\n",
            "[1, 31500/40625] Loss: 0.560\n",
            "[1, 31600/40625] Loss: 0.530\n",
            "[1, 31700/40625] Loss: 0.454\n",
            "[1, 31800/40625] Loss: 0.584\n",
            "[1, 31900/40625] Loss: 0.587\n",
            "[1, 32000/40625] Loss: 0.475\n",
            "[1, 32100/40625] Loss: 0.516\n",
            "[1, 32200/40625] Loss: 0.579\n",
            "[1, 32300/40625] Loss: 0.543\n",
            "[1, 32400/40625] Loss: 0.562\n",
            "[1, 32500/40625] Loss: 0.526\n",
            "[1, 32600/40625] Loss: 0.471\n",
            "[1, 32700/40625] Loss: 0.590\n",
            "[1, 32800/40625] Loss: 0.513\n",
            "[1, 32900/40625] Loss: 0.536\n",
            "[1, 33000/40625] Loss: 0.500\n",
            "[1, 33100/40625] Loss: 0.589\n",
            "[1, 33200/40625] Loss: 0.524\n",
            "[1, 33300/40625] Loss: 0.574\n",
            "[1, 33400/40625] Loss: 0.530\n",
            "[1, 33500/40625] Loss: 0.570\n",
            "[1, 33600/40625] Loss: 0.519\n",
            "[1, 33700/40625] Loss: 0.563\n",
            "[1, 33800/40625] Loss: 0.575\n",
            "[1, 33900/40625] Loss: 0.497\n",
            "[1, 34000/40625] Loss: 0.573\n",
            "[1, 34100/40625] Loss: 0.511\n",
            "[1, 34200/40625] Loss: 0.469\n",
            "[1, 34300/40625] Loss: 0.585\n",
            "[1, 34400/40625] Loss: 0.522\n",
            "[1, 34500/40625] Loss: 0.572\n",
            "[1, 34600/40625] Loss: 0.498\n",
            "[1, 34700/40625] Loss: 0.487\n",
            "[1, 34800/40625] Loss: 0.540\n",
            "[1, 34900/40625] Loss: 0.489\n",
            "[1, 35000/40625] Loss: 0.519\n",
            "[1, 35100/40625] Loss: 0.574\n",
            "[1, 35200/40625] Loss: 0.531\n",
            "[1, 35300/40625] Loss: 0.506\n",
            "[1, 35400/40625] Loss: 0.562\n",
            "[1, 35500/40625] Loss: 0.510\n",
            "[1, 35600/40625] Loss: 0.571\n",
            "[1, 35700/40625] Loss: 0.540\n",
            "[1, 35800/40625] Loss: 0.576\n",
            "[1, 35900/40625] Loss: 0.472\n",
            "[1, 36000/40625] Loss: 0.517\n",
            "[1, 36100/40625] Loss: 0.474\n",
            "[1, 36200/40625] Loss: 0.425\n",
            "[1, 36300/40625] Loss: 0.565\n",
            "[1, 36400/40625] Loss: 0.493\n",
            "[1, 36500/40625] Loss: 0.507\n",
            "[1, 36600/40625] Loss: 0.496\n",
            "[1, 36700/40625] Loss: 0.517\n",
            "[1, 36800/40625] Loss: 0.432\n",
            "[1, 36900/40625] Loss: 0.503\n",
            "[1, 37000/40625] Loss: 0.509\n",
            "[1, 37100/40625] Loss: 0.592\n",
            "[1, 37200/40625] Loss: 0.513\n",
            "[1, 37300/40625] Loss: 0.526\n",
            "[1, 37400/40625] Loss: 0.527\n",
            "[1, 37500/40625] Loss: 0.450\n",
            "[1, 37600/40625] Loss: 0.570\n",
            "[1, 37700/40625] Loss: 0.464\n",
            "[1, 37800/40625] Loss: 0.462\n",
            "[1, 37900/40625] Loss: 0.374\n",
            "[1, 38000/40625] Loss: 0.580\n",
            "[1, 38100/40625] Loss: 0.565\n",
            "[1, 38200/40625] Loss: 0.499\n",
            "[1, 38300/40625] Loss: 0.474\n",
            "[1, 38400/40625] Loss: 0.456\n",
            "[1, 38500/40625] Loss: 0.469\n",
            "[1, 38600/40625] Loss: 0.553\n",
            "[1, 38700/40625] Loss: 0.552\n",
            "[1, 38800/40625] Loss: 0.485\n",
            "[1, 38900/40625] Loss: 0.462\n",
            "[1, 39000/40625] Loss: 0.493\n",
            "[1, 39100/40625] Loss: 0.499\n",
            "[1, 39200/40625] Loss: 0.533\n",
            "[1, 39300/40625] Loss: 0.547\n",
            "[1, 39400/40625] Loss: 0.494\n",
            "[1, 39500/40625] Loss: 0.519\n",
            "[1, 39600/40625] Loss: 0.431\n",
            "[1, 39700/40625] Loss: 0.497\n",
            "[1, 39800/40625] Loss: 0.522\n",
            "[1, 39900/40625] Loss: 0.464\n",
            "[1, 40000/40625] Loss: 0.486\n",
            "[1, 40100/40625] Loss: 0.589\n",
            "[1, 40200/40625] Loss: 0.524\n",
            "[1, 40300/40625] Loss: 0.552\n",
            "[1, 40400/40625] Loss: 0.545\n",
            "[1, 40500/40625] Loss: 0.538\n",
            "[1, 40600/40625] Loss: 0.521\n",
            "[2,   100/40625] Loss: 0.430\n",
            "[2,   200/40625] Loss: 0.501\n",
            "[2,   300/40625] Loss: 0.483\n",
            "[2,   400/40625] Loss: 0.380\n",
            "[2,   500/40625] Loss: 0.412\n",
            "[2,   600/40625] Loss: 0.552\n",
            "[2,   700/40625] Loss: 0.413\n",
            "[2,   800/40625] Loss: 0.512\n",
            "[2,   900/40625] Loss: 0.629\n",
            "[2,  1000/40625] Loss: 0.499\n",
            "[2,  1100/40625] Loss: 0.459\n",
            "[2,  1200/40625] Loss: 0.510\n",
            "[2,  1300/40625] Loss: 0.399\n",
            "[2,  1400/40625] Loss: 0.508\n",
            "[2,  1500/40625] Loss: 0.444\n",
            "[2,  1600/40625] Loss: 0.439\n",
            "[2,  1700/40625] Loss: 0.414\n",
            "[2,  1800/40625] Loss: 0.596\n",
            "[2,  1900/40625] Loss: 0.474\n",
            "[2,  2000/40625] Loss: 0.353\n",
            "[2,  2100/40625] Loss: 0.413\n",
            "[2,  2200/40625] Loss: 0.531\n",
            "[2,  2300/40625] Loss: 0.521\n",
            "[2,  2400/40625] Loss: 0.471\n",
            "[2,  2500/40625] Loss: 0.495\n",
            "[2,  2600/40625] Loss: 0.519\n",
            "[2,  2700/40625] Loss: 0.435\n",
            "[2,  2800/40625] Loss: 0.472\n",
            "[2,  2900/40625] Loss: 0.484\n",
            "[2,  3000/40625] Loss: 0.372\n",
            "[2,  3100/40625] Loss: 0.515\n",
            "[2,  3200/40625] Loss: 0.447\n",
            "[2,  3300/40625] Loss: 0.459\n",
            "[2,  3400/40625] Loss: 0.426\n",
            "[2,  3500/40625] Loss: 0.504\n",
            "[2,  3600/40625] Loss: 0.413\n",
            "[2,  3700/40625] Loss: 0.471\n",
            "[2,  3800/40625] Loss: 0.460\n",
            "[2,  3900/40625] Loss: 0.443\n",
            "[2,  4000/40625] Loss: 0.483\n",
            "[2,  4100/40625] Loss: 0.518\n",
            "[2,  4200/40625] Loss: 0.455\n",
            "[2,  4300/40625] Loss: 0.387\n",
            "[2,  4400/40625] Loss: 0.361\n",
            "[2,  4500/40625] Loss: 0.504\n",
            "[2,  4600/40625] Loss: 0.432\n",
            "[2,  4700/40625] Loss: 0.468\n",
            "[2,  4800/40625] Loss: 0.445\n",
            "[2,  4900/40625] Loss: 0.447\n",
            "[2,  5000/40625] Loss: 0.467\n",
            "[2,  5100/40625] Loss: 0.480\n",
            "[2,  5200/40625] Loss: 0.506\n",
            "[2,  5300/40625] Loss: 0.567\n",
            "[2,  5400/40625] Loss: 0.507\n",
            "[2,  5500/40625] Loss: 0.461\n",
            "[2,  5600/40625] Loss: 0.417\n",
            "[2,  5700/40625] Loss: 0.524\n",
            "[2,  5800/40625] Loss: 0.480\n",
            "[2,  5900/40625] Loss: 0.356\n",
            "[2,  6000/40625] Loss: 0.540\n",
            "[2,  6100/40625] Loss: 0.474\n",
            "[2,  6200/40625] Loss: 0.391\n",
            "[2,  6300/40625] Loss: 0.446\n",
            "[2,  6400/40625] Loss: 0.494\n",
            "[2,  6500/40625] Loss: 0.382\n",
            "[2,  6600/40625] Loss: 0.351\n",
            "[2,  6700/40625] Loss: 0.348\n",
            "[2,  6800/40625] Loss: 0.367\n",
            "[2,  6900/40625] Loss: 0.336\n",
            "[2,  7000/40625] Loss: 0.400\n",
            "[2,  7100/40625] Loss: 0.487\n",
            "[2,  7200/40625] Loss: 0.494\n",
            "[2,  7300/40625] Loss: 0.517\n",
            "[2,  7400/40625] Loss: 0.448\n",
            "[2,  7500/40625] Loss: 0.483\n",
            "[2,  7600/40625] Loss: 0.429\n",
            "[2,  7700/40625] Loss: 0.398\n",
            "[2,  7800/40625] Loss: 0.580\n",
            "[2,  7900/40625] Loss: 0.375\n",
            "[2,  8000/40625] Loss: 0.419\n",
            "[2,  8100/40625] Loss: 0.428\n",
            "[2,  8200/40625] Loss: 0.418\n",
            "[2,  8300/40625] Loss: 0.445\n",
            "[2,  8400/40625] Loss: 0.342\n",
            "[2,  8500/40625] Loss: 0.537\n",
            "[2,  8600/40625] Loss: 0.528\n",
            "[2,  8700/40625] Loss: 0.548\n",
            "[2,  8800/40625] Loss: 0.427\n",
            "[2,  8900/40625] Loss: 0.419\n",
            "[2,  9000/40625] Loss: 0.400\n",
            "[2,  9100/40625] Loss: 0.393\n",
            "[2,  9200/40625] Loss: 0.425\n",
            "[2,  9300/40625] Loss: 0.506\n",
            "[2,  9400/40625] Loss: 0.299\n",
            "[2,  9500/40625] Loss: 0.517\n",
            "[2,  9600/40625] Loss: 0.369\n",
            "[2,  9700/40625] Loss: 0.464\n",
            "[2,  9800/40625] Loss: 0.445\n",
            "[2,  9900/40625] Loss: 0.481\n",
            "[2, 10000/40625] Loss: 0.457\n",
            "[2, 10100/40625] Loss: 0.440\n",
            "[2, 10200/40625] Loss: 0.348\n",
            "[2, 10300/40625] Loss: 0.401\n",
            "[2, 10400/40625] Loss: 0.452\n",
            "[2, 10500/40625] Loss: 0.394\n",
            "[2, 10600/40625] Loss: 0.409\n",
            "[2, 10700/40625] Loss: 0.502\n",
            "[2, 10800/40625] Loss: 0.453\n",
            "[2, 10900/40625] Loss: 0.482\n",
            "[2, 11000/40625] Loss: 0.431\n",
            "[2, 11100/40625] Loss: 0.447\n",
            "[2, 11200/40625] Loss: 0.402\n",
            "[2, 11300/40625] Loss: 0.487\n",
            "[2, 11400/40625] Loss: 0.419\n",
            "[2, 11500/40625] Loss: 0.540\n",
            "[2, 11600/40625] Loss: 0.375\n",
            "[2, 11700/40625] Loss: 0.379\n",
            "[2, 11800/40625] Loss: 0.500\n",
            "[2, 11900/40625] Loss: 0.376\n",
            "[2, 12000/40625] Loss: 0.486\n",
            "[2, 12100/40625] Loss: 0.405\n",
            "[2, 12200/40625] Loss: 0.477\n",
            "[2, 12300/40625] Loss: 0.429\n",
            "[2, 12400/40625] Loss: 0.370\n",
            "[2, 12500/40625] Loss: 0.455\n",
            "[2, 12600/40625] Loss: 0.469\n",
            "[2, 12700/40625] Loss: 0.388\n",
            "[2, 12800/40625] Loss: 0.355\n",
            "[2, 12900/40625] Loss: 0.538\n",
            "[2, 13000/40625] Loss: 0.370\n",
            "[2, 13100/40625] Loss: 0.487\n",
            "[2, 13200/40625] Loss: 0.433\n",
            "[2, 13300/40625] Loss: 0.405\n",
            "[2, 13400/40625] Loss: 0.515\n",
            "[2, 13500/40625] Loss: 0.494\n",
            "[2, 13600/40625] Loss: 0.455\n",
            "[2, 13700/40625] Loss: 0.482\n",
            "[2, 13800/40625] Loss: 0.430\n",
            "[2, 13900/40625] Loss: 0.385\n",
            "[2, 14000/40625] Loss: 0.347\n",
            "[2, 14100/40625] Loss: 0.421\n",
            "[2, 14200/40625] Loss: 0.504\n",
            "[2, 14300/40625] Loss: 0.338\n",
            "[2, 14400/40625] Loss: 0.473\n",
            "[2, 14500/40625] Loss: 0.489\n",
            "[2, 14600/40625] Loss: 0.333\n",
            "[2, 14700/40625] Loss: 0.403\n",
            "[2, 14800/40625] Loss: 0.355\n",
            "[2, 14900/40625] Loss: 0.440\n",
            "[2, 15000/40625] Loss: 0.389\n",
            "[2, 15100/40625] Loss: 0.281\n",
            "[2, 15200/40625] Loss: 0.489\n",
            "[2, 15300/40625] Loss: 0.472\n",
            "[2, 15400/40625] Loss: 0.366\n",
            "[2, 15500/40625] Loss: 0.351\n",
            "[2, 15600/40625] Loss: 0.457\n",
            "[2, 15700/40625] Loss: 0.407\n",
            "[2, 15800/40625] Loss: 0.353\n",
            "[2, 15900/40625] Loss: 0.384\n",
            "[2, 16000/40625] Loss: 0.392\n",
            "[2, 16100/40625] Loss: 0.435\n",
            "[2, 16200/40625] Loss: 0.281\n",
            "[2, 16300/40625] Loss: 0.500\n",
            "[2, 16400/40625] Loss: 0.379\n",
            "[2, 16500/40625] Loss: 0.386\n",
            "[2, 16600/40625] Loss: 0.371\n",
            "[2, 16700/40625] Loss: 0.422\n",
            "[2, 16800/40625] Loss: 0.446\n",
            "[2, 16900/40625] Loss: 0.379\n",
            "[2, 17000/40625] Loss: 0.458\n",
            "[2, 17100/40625] Loss: 0.457\n",
            "[2, 17200/40625] Loss: 0.474\n",
            "[2, 17300/40625] Loss: 0.390\n",
            "[2, 17400/40625] Loss: 0.428\n",
            "[2, 17500/40625] Loss: 0.530\n",
            "[2, 17600/40625] Loss: 0.458\n",
            "[2, 17700/40625] Loss: 0.389\n",
            "[2, 17800/40625] Loss: 0.358\n",
            "[2, 17900/40625] Loss: 0.472\n",
            "[2, 18000/40625] Loss: 0.406\n",
            "[2, 18100/40625] Loss: 0.326\n",
            "[2, 18200/40625] Loss: 0.437\n",
            "[2, 18300/40625] Loss: 0.372\n",
            "[2, 18400/40625] Loss: 0.356\n",
            "[2, 18500/40625] Loss: 0.455\n",
            "[2, 18600/40625] Loss: 0.380\n",
            "[2, 18700/40625] Loss: 0.463\n",
            "[2, 18800/40625] Loss: 0.468\n",
            "[2, 18900/40625] Loss: 0.412\n",
            "[2, 19000/40625] Loss: 0.417\n",
            "[2, 19100/40625] Loss: 0.411\n",
            "[2, 19200/40625] Loss: 0.415\n",
            "[2, 19300/40625] Loss: 0.435\n",
            "[2, 19400/40625] Loss: 0.414\n",
            "[2, 19500/40625] Loss: 0.411\n",
            "[2, 19600/40625] Loss: 0.389\n",
            "[2, 19700/40625] Loss: 0.502\n",
            "[2, 19800/40625] Loss: 0.376\n",
            "[2, 19900/40625] Loss: 0.376\n",
            "[2, 20000/40625] Loss: 0.487\n",
            "[2, 20100/40625] Loss: 0.327\n",
            "[2, 20200/40625] Loss: 0.392\n",
            "[2, 20300/40625] Loss: 0.348\n",
            "[2, 20400/40625] Loss: 0.442\n",
            "[2, 20500/40625] Loss: 0.422\n",
            "[2, 20600/40625] Loss: 0.455\n",
            "[2, 20700/40625] Loss: 0.352\n",
            "[2, 20800/40625] Loss: 0.547\n",
            "[2, 20900/40625] Loss: 0.525\n",
            "[2, 21000/40625] Loss: 0.328\n",
            "[2, 21100/40625] Loss: 0.381\n",
            "[2, 21200/40625] Loss: 0.347\n",
            "[2, 21300/40625] Loss: 0.361\n",
            "[2, 21400/40625] Loss: 0.426\n",
            "[2, 21500/40625] Loss: 0.476\n",
            "[2, 21600/40625] Loss: 0.395\n",
            "[2, 21700/40625] Loss: 0.467\n",
            "[2, 21800/40625] Loss: 0.378\n",
            "[2, 21900/40625] Loss: 0.374\n",
            "[2, 22000/40625] Loss: 0.456\n",
            "[2, 22100/40625] Loss: 0.340\n",
            "[2, 22200/40625] Loss: 0.388\n",
            "[2, 22300/40625] Loss: 0.401\n",
            "[2, 22400/40625] Loss: 0.392\n",
            "[2, 22500/40625] Loss: 0.429\n",
            "[2, 22600/40625] Loss: 0.344\n",
            "[2, 22700/40625] Loss: 0.447\n",
            "[2, 22800/40625] Loss: 0.378\n",
            "[2, 22900/40625] Loss: 0.381\n",
            "[2, 23000/40625] Loss: 0.356\n",
            "[2, 23100/40625] Loss: 0.312\n",
            "[2, 23200/40625] Loss: 0.319\n",
            "[2, 23300/40625] Loss: 0.381\n",
            "[2, 23400/40625] Loss: 0.420\n",
            "[2, 23500/40625] Loss: 0.413\n",
            "[2, 23600/40625] Loss: 0.407\n",
            "[2, 23700/40625] Loss: 0.419\n",
            "[2, 23800/40625] Loss: 0.427\n",
            "[2, 23900/40625] Loss: 0.289\n",
            "[2, 24000/40625] Loss: 0.349\n",
            "[2, 24100/40625] Loss: 0.314\n",
            "[2, 24200/40625] Loss: 0.235\n",
            "[2, 24300/40625] Loss: 0.386\n",
            "[2, 24400/40625] Loss: 0.313\n",
            "[2, 24500/40625] Loss: 0.563\n",
            "[2, 24600/40625] Loss: 0.412\n",
            "[2, 24700/40625] Loss: 0.324\n",
            "[2, 24800/40625] Loss: 0.383\n",
            "[2, 24900/40625] Loss: 0.403\n",
            "[2, 25000/40625] Loss: 0.419\n",
            "[2, 25100/40625] Loss: 0.266\n",
            "[2, 25200/40625] Loss: 0.441\n",
            "[2, 25300/40625] Loss: 0.393\n",
            "[2, 25400/40625] Loss: 0.361\n",
            "[2, 25500/40625] Loss: 0.466\n",
            "[2, 25600/40625] Loss: 0.486\n",
            "[2, 25700/40625] Loss: 0.338\n",
            "[2, 25800/40625] Loss: 0.373\n",
            "[2, 25900/40625] Loss: 0.303\n",
            "[2, 26000/40625] Loss: 0.355\n",
            "[2, 26100/40625] Loss: 0.350\n",
            "[2, 26200/40625] Loss: 0.399\n",
            "[2, 26300/40625] Loss: 0.268\n",
            "[2, 26400/40625] Loss: 0.297\n",
            "[2, 26500/40625] Loss: 0.524\n",
            "[2, 26600/40625] Loss: 0.429\n",
            "[2, 26700/40625] Loss: 0.376\n",
            "[2, 26800/40625] Loss: 0.390\n",
            "[2, 26900/40625] Loss: 0.371\n",
            "[2, 27000/40625] Loss: 0.426\n",
            "[2, 27100/40625] Loss: 0.409\n",
            "[2, 27200/40625] Loss: 0.446\n",
            "[2, 27300/40625] Loss: 0.537\n",
            "[2, 27400/40625] Loss: 0.302\n",
            "[2, 27500/40625] Loss: 0.408\n",
            "[2, 27600/40625] Loss: 0.368\n",
            "[2, 27700/40625] Loss: 0.331\n",
            "[2, 27800/40625] Loss: 0.371\n",
            "[2, 27900/40625] Loss: 0.342\n",
            "[2, 28000/40625] Loss: 0.553\n",
            "[2, 28100/40625] Loss: 0.429\n",
            "[2, 28200/40625] Loss: 0.450\n",
            "[2, 28300/40625] Loss: 0.455\n",
            "[2, 28400/40625] Loss: 0.389\n",
            "[2, 28500/40625] Loss: 0.414\n",
            "[2, 28600/40625] Loss: 0.367\n",
            "[2, 28700/40625] Loss: 0.360\n",
            "[2, 28800/40625] Loss: 0.504\n",
            "[2, 28900/40625] Loss: 0.461\n",
            "[2, 29000/40625] Loss: 0.341\n",
            "[2, 29100/40625] Loss: 0.313\n",
            "[2, 29200/40625] Loss: 0.388\n",
            "[2, 29300/40625] Loss: 0.408\n",
            "[2, 29400/40625] Loss: 0.474\n",
            "[2, 29500/40625] Loss: 0.434\n",
            "[2, 29600/40625] Loss: 0.299\n",
            "[2, 29700/40625] Loss: 0.442\n",
            "[2, 29800/40625] Loss: 0.435\n",
            "[2, 29900/40625] Loss: 0.314\n",
            "[2, 30000/40625] Loss: 0.321\n",
            "[2, 30100/40625] Loss: 0.357\n",
            "[2, 30200/40625] Loss: 0.426\n",
            "[2, 30300/40625] Loss: 0.453\n",
            "[2, 30400/40625] Loss: 0.349\n",
            "[2, 30500/40625] Loss: 0.311\n",
            "[2, 30600/40625] Loss: 0.277\n",
            "[2, 30700/40625] Loss: 0.349\n",
            "[2, 30800/40625] Loss: 0.520\n",
            "[2, 30900/40625] Loss: 0.444\n",
            "[2, 31000/40625] Loss: 0.365\n",
            "[2, 31100/40625] Loss: 0.376\n",
            "[2, 31200/40625] Loss: 0.389\n",
            "[2, 31300/40625] Loss: 0.321\n",
            "[2, 31400/40625] Loss: 0.305\n",
            "[2, 31500/40625] Loss: 0.390\n",
            "[2, 31600/40625] Loss: 0.410\n",
            "[2, 31700/40625] Loss: 0.342\n",
            "[2, 31800/40625] Loss: 0.454\n",
            "[2, 31900/40625] Loss: 0.450\n",
            "[2, 32000/40625] Loss: 0.390\n",
            "[2, 32100/40625] Loss: 0.308\n",
            "[2, 32200/40625] Loss: 0.386\n",
            "[2, 32300/40625] Loss: 0.353\n",
            "[2, 32400/40625] Loss: 0.369\n",
            "[2, 32500/40625] Loss: 0.365\n",
            "[2, 32600/40625] Loss: 0.335\n",
            "[2, 32700/40625] Loss: 0.429\n",
            "[2, 32800/40625] Loss: 0.311\n",
            "[2, 32900/40625] Loss: 0.453\n",
            "[2, 33000/40625] Loss: 0.454\n",
            "[2, 33100/40625] Loss: 0.419\n",
            "[2, 33200/40625] Loss: 0.353\n",
            "[2, 33300/40625] Loss: 0.388\n",
            "[2, 33400/40625] Loss: 0.398\n",
            "[2, 33500/40625] Loss: 0.392\n",
            "[2, 33600/40625] Loss: 0.392\n",
            "[2, 33700/40625] Loss: 0.428\n",
            "[2, 33800/40625] Loss: 0.415\n",
            "[2, 33900/40625] Loss: 0.307\n",
            "[2, 34000/40625] Loss: 0.514\n",
            "[2, 34100/40625] Loss: 0.397\n",
            "[2, 34200/40625] Loss: 0.305\n",
            "[2, 34300/40625] Loss: 0.439\n",
            "[2, 34400/40625] Loss: 0.412\n",
            "[2, 34500/40625] Loss: 0.419\n",
            "[2, 34600/40625] Loss: 0.332\n",
            "[2, 34700/40625] Loss: 0.291\n",
            "[2, 34800/40625] Loss: 0.378\n",
            "[2, 34900/40625] Loss: 0.405\n",
            "[2, 35000/40625] Loss: 0.409\n",
            "[2, 35100/40625] Loss: 0.344\n",
            "[2, 35200/40625] Loss: 0.364\n",
            "[2, 35300/40625] Loss: 0.401\n",
            "[2, 35400/40625] Loss: 0.419\n",
            "[2, 35500/40625] Loss: 0.365\n",
            "[2, 35600/40625] Loss: 0.374\n",
            "[2, 35700/40625] Loss: 0.437\n",
            "[2, 35800/40625] Loss: 0.396\n",
            "[2, 35900/40625] Loss: 0.345\n",
            "[2, 36000/40625] Loss: 0.294\n",
            "[2, 36100/40625] Loss: 0.322\n",
            "[2, 36200/40625] Loss: 0.282\n",
            "[2, 36300/40625] Loss: 0.502\n",
            "[2, 36400/40625] Loss: 0.402\n",
            "[2, 36500/40625] Loss: 0.372\n",
            "[2, 36600/40625] Loss: 0.358\n",
            "[2, 36700/40625] Loss: 0.393\n",
            "[2, 36800/40625] Loss: 0.338\n",
            "[2, 36900/40625] Loss: 0.370\n",
            "[2, 37000/40625] Loss: 0.290\n",
            "[2, 37100/40625] Loss: 0.391\n",
            "[2, 37200/40625] Loss: 0.445\n",
            "[2, 37300/40625] Loss: 0.462\n",
            "[2, 37400/40625] Loss: 0.438\n",
            "[2, 37500/40625] Loss: 0.357\n",
            "[2, 37600/40625] Loss: 0.453\n",
            "[2, 37700/40625] Loss: 0.350\n",
            "[2, 37800/40625] Loss: 0.347\n",
            "[2, 37900/40625] Loss: 0.276\n",
            "[2, 38000/40625] Loss: 0.305\n",
            "[2, 38100/40625] Loss: 0.396\n",
            "[2, 38200/40625] Loss: 0.379\n",
            "[2, 38300/40625] Loss: 0.354\n",
            "[2, 38400/40625] Loss: 0.359\n",
            "[2, 38500/40625] Loss: 0.344\n",
            "[2, 38600/40625] Loss: 0.368\n",
            "[2, 38700/40625] Loss: 0.372\n",
            "[2, 38800/40625] Loss: 0.395\n",
            "[2, 38900/40625] Loss: 0.303\n",
            "[2, 39000/40625] Loss: 0.276\n",
            "[2, 39100/40625] Loss: 0.363\n",
            "[2, 39200/40625] Loss: 0.375\n",
            "[2, 39300/40625] Loss: 0.366\n",
            "[2, 39400/40625] Loss: 0.324\n",
            "[2, 39500/40625] Loss: 0.418\n",
            "[2, 39600/40625] Loss: 0.292\n",
            "[2, 39700/40625] Loss: 0.418\n",
            "[2, 39800/40625] Loss: 0.419\n",
            "[2, 39900/40625] Loss: 0.361\n",
            "[2, 40000/40625] Loss: 0.337\n",
            "[2, 40100/40625] Loss: 0.402\n",
            "[2, 40200/40625] Loss: 0.464\n",
            "[2, 40300/40625] Loss: 0.461\n",
            "[2, 40400/40625] Loss: 0.380\n",
            "[2, 40500/40625] Loss: 0.385\n",
            "[2, 40600/40625] Loss: 0.364\n",
            "[3,   100/40625] Loss: 0.287\n",
            "[3,   200/40625] Loss: 0.368\n",
            "[3,   300/40625] Loss: 0.381\n",
            "[3,   400/40625] Loss: 0.248\n",
            "[3,   500/40625] Loss: 0.335\n",
            "[3,   600/40625] Loss: 0.366\n",
            "[3,   700/40625] Loss: 0.238\n",
            "[3,   800/40625] Loss: 0.441\n",
            "[3,   900/40625] Loss: 0.538\n",
            "[3,  1000/40625] Loss: 0.408\n",
            "[3,  1100/40625] Loss: 0.341\n",
            "[3,  1200/40625] Loss: 0.369\n",
            "[3,  1300/40625] Loss: 0.270\n",
            "[3,  1400/40625] Loss: 0.346\n",
            "[3,  1500/40625] Loss: 0.399\n",
            "[3,  1600/40625] Loss: 0.384\n",
            "[3,  1700/40625] Loss: 0.305\n",
            "[3,  1800/40625] Loss: 0.455\n",
            "[3,  1900/40625] Loss: 0.263\n",
            "[3,  2000/40625] Loss: 0.260\n",
            "[3,  2100/40625] Loss: 0.285\n",
            "[3,  2200/40625] Loss: 0.297\n",
            "[3,  2300/40625] Loss: 0.466\n",
            "[3,  2400/40625] Loss: 0.276\n",
            "[3,  2500/40625] Loss: 0.470\n",
            "[3,  2600/40625] Loss: 0.354\n",
            "[3,  2700/40625] Loss: 0.305\n",
            "[3,  2800/40625] Loss: 0.443\n",
            "[3,  2900/40625] Loss: 0.395\n",
            "[3,  3000/40625] Loss: 0.264\n",
            "[3,  3100/40625] Loss: 0.478\n",
            "[3,  3200/40625] Loss: 0.276\n",
            "[3,  3300/40625] Loss: 0.352\n",
            "[3,  3400/40625] Loss: 0.417\n",
            "[3,  3500/40625] Loss: 0.428\n",
            "[3,  3600/40625] Loss: 0.264\n",
            "[3,  3700/40625] Loss: 0.364\n",
            "[3,  3800/40625] Loss: 0.448\n",
            "[3,  3900/40625] Loss: 0.319\n",
            "[3,  4000/40625] Loss: 0.337\n",
            "[3,  4100/40625] Loss: 0.383\n",
            "[3,  4200/40625] Loss: 0.386\n",
            "[3,  4300/40625] Loss: 0.316\n",
            "[3,  4400/40625] Loss: 0.323\n",
            "[3,  4500/40625] Loss: 0.413\n",
            "[3,  4600/40625] Loss: 0.325\n",
            "[3,  4700/40625] Loss: 0.357\n",
            "[3,  4800/40625] Loss: 0.339\n",
            "[3,  4900/40625] Loss: 0.349\n",
            "[3,  5000/40625] Loss: 0.327\n",
            "[3,  5100/40625] Loss: 0.339\n",
            "[3,  5200/40625] Loss: 0.334\n",
            "[3,  5300/40625] Loss: 0.369\n",
            "[3,  5400/40625] Loss: 0.431\n",
            "[3,  5500/40625] Loss: 0.345\n",
            "[3,  5600/40625] Loss: 0.266\n",
            "[3,  5700/40625] Loss: 0.486\n",
            "[3,  5800/40625] Loss: 0.385\n",
            "[3,  5900/40625] Loss: 0.293\n",
            "[3,  6000/40625] Loss: 0.400\n",
            "[3,  6100/40625] Loss: 0.384\n",
            "[3,  6200/40625] Loss: 0.268\n",
            "[3,  6300/40625] Loss: 0.397\n",
            "[3,  6400/40625] Loss: 0.348\n",
            "[3,  6500/40625] Loss: 0.280\n",
            "[3,  6600/40625] Loss: 0.211\n",
            "[3,  6700/40625] Loss: 0.263\n",
            "[3,  6800/40625] Loss: 0.259\n",
            "[3,  6900/40625] Loss: 0.261\n",
            "[3,  7000/40625] Loss: 0.284\n",
            "[3,  7100/40625] Loss: 0.358\n",
            "[3,  7200/40625] Loss: 0.412\n",
            "[3,  7300/40625] Loss: 0.355\n",
            "[3,  7400/40625] Loss: 0.323\n",
            "[3,  7500/40625] Loss: 0.318\n",
            "[3,  7600/40625] Loss: 0.348\n",
            "[3,  7700/40625] Loss: 0.344\n",
            "[3,  7800/40625] Loss: 0.494\n",
            "[3,  7900/40625] Loss: 0.284\n",
            "[3,  8000/40625] Loss: 0.346\n",
            "[3,  8100/40625] Loss: 0.319\n",
            "[3,  8200/40625] Loss: 0.386\n",
            "[3,  8300/40625] Loss: 0.349\n",
            "[3,  8400/40625] Loss: 0.327\n",
            "[3,  8500/40625] Loss: 0.393\n",
            "[3,  8600/40625] Loss: 0.398\n",
            "[3,  8700/40625] Loss: 0.408\n",
            "[3,  8800/40625] Loss: 0.307\n",
            "[3,  8900/40625] Loss: 0.369\n",
            "[3,  9000/40625] Loss: 0.316\n",
            "[3,  9100/40625] Loss: 0.325\n",
            "[3,  9200/40625] Loss: 0.304\n",
            "[3,  9300/40625] Loss: 0.370\n",
            "[3,  9400/40625] Loss: 0.249\n",
            "[3,  9500/40625] Loss: 0.386\n",
            "[3,  9600/40625] Loss: 0.292\n",
            "[3,  9700/40625] Loss: 0.344\n",
            "[3,  9800/40625] Loss: 0.313\n",
            "[3,  9900/40625] Loss: 0.448\n",
            "[3, 10000/40625] Loss: 0.357\n",
            "[3, 10100/40625] Loss: 0.354\n",
            "[3, 10200/40625] Loss: 0.294\n",
            "[3, 10300/40625] Loss: 0.372\n",
            "[3, 10400/40625] Loss: 0.337\n",
            "[3, 10500/40625] Loss: 0.328\n",
            "[3, 10600/40625] Loss: 0.357\n",
            "[3, 10700/40625] Loss: 0.429\n",
            "[3, 10800/40625] Loss: 0.335\n",
            "[3, 10900/40625] Loss: 0.374\n",
            "[3, 11000/40625] Loss: 0.318\n",
            "[3, 11100/40625] Loss: 0.441\n",
            "[3, 11200/40625] Loss: 0.374\n",
            "[3, 11300/40625] Loss: 0.376\n",
            "[3, 11400/40625] Loss: 0.291\n",
            "[3, 11500/40625] Loss: 0.476\n",
            "[3, 11600/40625] Loss: 0.287\n",
            "[3, 11700/40625] Loss: 0.314\n",
            "[3, 11800/40625] Loss: 0.371\n",
            "[3, 11900/40625] Loss: 0.285\n",
            "[3, 12000/40625] Loss: 0.460\n",
            "[3, 12100/40625] Loss: 0.371\n",
            "[3, 12200/40625] Loss: 0.415\n",
            "[3, 12300/40625] Loss: 0.368\n",
            "[3, 12400/40625] Loss: 0.279\n",
            "[3, 12500/40625] Loss: 0.335\n",
            "[3, 12600/40625] Loss: 0.353\n",
            "[3, 12700/40625] Loss: 0.288\n",
            "[3, 12800/40625] Loss: 0.243\n",
            "[3, 12900/40625] Loss: 0.459\n",
            "[3, 13000/40625] Loss: 0.385\n",
            "[3, 13100/40625] Loss: 0.351\n",
            "[3, 13200/40625] Loss: 0.324\n",
            "[3, 13300/40625] Loss: 0.360\n",
            "[3, 13400/40625] Loss: 0.483\n",
            "[3, 13500/40625] Loss: 0.404\n",
            "[3, 13600/40625] Loss: 0.436\n",
            "[3, 13700/40625] Loss: 0.426\n",
            "[3, 13800/40625] Loss: 0.366\n",
            "[3, 13900/40625] Loss: 0.277\n",
            "[3, 14000/40625] Loss: 0.261\n",
            "[3, 14100/40625] Loss: 0.399\n",
            "[3, 14200/40625] Loss: 0.413\n",
            "[3, 14300/40625] Loss: 0.311\n",
            "[3, 14400/40625] Loss: 0.356\n",
            "[3, 14500/40625] Loss: 0.345\n",
            "[3, 14600/40625] Loss: 0.177\n",
            "[3, 14700/40625] Loss: 0.319\n",
            "[3, 14800/40625] Loss: 0.284\n",
            "[3, 14900/40625] Loss: 0.329\n",
            "[3, 15000/40625] Loss: 0.364\n",
            "[3, 15100/40625] Loss: 0.255\n",
            "[3, 15200/40625] Loss: 0.313\n",
            "[3, 15300/40625] Loss: 0.352\n",
            "[3, 15400/40625] Loss: 0.338\n",
            "[3, 15500/40625] Loss: 0.331\n",
            "[3, 15600/40625] Loss: 0.402\n",
            "[3, 15700/40625] Loss: 0.374\n",
            "[3, 15800/40625] Loss: 0.280\n",
            "[3, 15900/40625] Loss: 0.289\n",
            "[3, 16000/40625] Loss: 0.312\n",
            "[3, 16100/40625] Loss: 0.376\n",
            "[3, 16200/40625] Loss: 0.201\n",
            "[3, 16300/40625] Loss: 0.331\n",
            "[3, 16400/40625] Loss: 0.317\n",
            "[3, 16500/40625] Loss: 0.327\n",
            "[3, 16600/40625] Loss: 0.289\n",
            "[3, 16700/40625] Loss: 0.328\n",
            "[3, 16800/40625] Loss: 0.402\n",
            "[3, 16900/40625] Loss: 0.319\n",
            "[3, 17000/40625] Loss: 0.344\n",
            "[3, 17100/40625] Loss: 0.395\n",
            "[3, 17200/40625] Loss: 0.363\n",
            "[3, 17300/40625] Loss: 0.316\n",
            "[3, 17400/40625] Loss: 0.339\n",
            "[3, 17500/40625] Loss: 0.371\n",
            "[3, 17600/40625] Loss: 0.419\n",
            "[3, 17700/40625] Loss: 0.341\n",
            "[3, 17800/40625] Loss: 0.249\n",
            "[3, 17900/40625] Loss: 0.430\n",
            "[3, 18000/40625] Loss: 0.315\n",
            "[3, 18100/40625] Loss: 0.244\n",
            "[3, 18200/40625] Loss: 0.380\n",
            "[3, 18300/40625] Loss: 0.228\n",
            "[3, 18400/40625] Loss: 0.276\n",
            "[3, 18500/40625] Loss: 0.366\n",
            "[3, 18600/40625] Loss: 0.362\n",
            "[3, 18700/40625] Loss: 0.327\n",
            "[3, 18800/40625] Loss: 0.452\n",
            "[3, 18900/40625] Loss: 0.344\n",
            "[3, 19000/40625] Loss: 0.326\n",
            "[3, 19100/40625] Loss: 0.359\n",
            "[3, 19200/40625] Loss: 0.326\n",
            "[3, 19300/40625] Loss: 0.308\n",
            "[3, 19400/40625] Loss: 0.332\n",
            "[3, 19500/40625] Loss: 0.336\n",
            "[3, 19600/40625] Loss: 0.314\n",
            "[3, 19700/40625] Loss: 0.354\n",
            "[3, 19800/40625] Loss: 0.338\n",
            "[3, 19900/40625] Loss: 0.322\n",
            "[3, 20000/40625] Loss: 0.427\n",
            "[3, 20100/40625] Loss: 0.275\n",
            "[3, 20200/40625] Loss: 0.328\n",
            "[3, 20300/40625] Loss: 0.347\n",
            "[3, 20400/40625] Loss: 0.313\n",
            "[3, 20500/40625] Loss: 0.351\n",
            "[3, 20600/40625] Loss: 0.348\n",
            "[3, 20700/40625] Loss: 0.425\n",
            "[3, 20800/40625] Loss: 0.421\n",
            "[3, 20900/40625] Loss: 0.461\n",
            "[3, 21000/40625] Loss: 0.273\n",
            "[3, 21100/40625] Loss: 0.384\n",
            "[3, 21200/40625] Loss: 0.257\n",
            "[3, 21300/40625] Loss: 0.352\n",
            "[3, 21400/40625] Loss: 0.263\n",
            "[3, 21500/40625] Loss: 0.388\n",
            "[3, 21600/40625] Loss: 0.410\n",
            "[3, 21700/40625] Loss: 0.383\n",
            "[3, 21800/40625] Loss: 0.255\n",
            "[3, 21900/40625] Loss: 0.312\n",
            "[3, 22000/40625] Loss: 0.427\n",
            "[3, 22100/40625] Loss: 0.346\n",
            "[3, 22200/40625] Loss: 0.422\n",
            "[3, 22300/40625] Loss: 0.341\n",
            "[3, 22400/40625] Loss: 0.324\n",
            "[3, 22500/40625] Loss: 0.342\n",
            "[3, 22600/40625] Loss: 0.256\n",
            "[3, 22700/40625] Loss: 0.410\n",
            "[3, 22800/40625] Loss: 0.329\n",
            "[3, 22900/40625] Loss: 0.271\n",
            "[3, 23000/40625] Loss: 0.312\n",
            "[3, 23100/40625] Loss: 0.215\n",
            "[3, 23200/40625] Loss: 0.412\n",
            "[3, 23300/40625] Loss: 0.264\n",
            "[3, 23400/40625] Loss: 0.348\n",
            "[3, 23500/40625] Loss: 0.343\n",
            "[3, 23600/40625] Loss: 0.390\n",
            "[3, 23700/40625] Loss: 0.325\n",
            "[3, 23800/40625] Loss: 0.341\n",
            "[3, 23900/40625] Loss: 0.230\n",
            "[3, 24000/40625] Loss: 0.329\n",
            "[3, 24100/40625] Loss: 0.277\n",
            "[3, 24200/40625] Loss: 0.203\n",
            "[3, 24300/40625] Loss: 0.321\n",
            "[3, 24400/40625] Loss: 0.291\n",
            "[3, 24500/40625] Loss: 0.469\n",
            "[3, 24600/40625] Loss: 0.286\n",
            "[3, 24700/40625] Loss: 0.249\n",
            "[3, 24800/40625] Loss: 0.385\n",
            "[3, 24900/40625] Loss: 0.302\n",
            "[3, 25000/40625] Loss: 0.439\n",
            "[3, 25100/40625] Loss: 0.209\n",
            "[3, 25200/40625] Loss: 0.324\n",
            "[3, 25300/40625] Loss: 0.361\n",
            "[3, 25400/40625] Loss: 0.278\n",
            "[3, 25500/40625] Loss: 0.384\n",
            "[3, 25600/40625] Loss: 0.433\n",
            "[3, 25700/40625] Loss: 0.337\n",
            "[3, 25800/40625] Loss: 0.355\n",
            "[3, 25900/40625] Loss: 0.286\n",
            "[3, 26000/40625] Loss: 0.277\n",
            "[3, 26100/40625] Loss: 0.319\n",
            "[3, 26200/40625] Loss: 0.314\n",
            "[3, 26300/40625] Loss: 0.319\n",
            "[3, 26400/40625] Loss: 0.252\n",
            "[3, 26500/40625] Loss: 0.428\n",
            "[3, 26600/40625] Loss: 0.422\n",
            "[3, 26700/40625] Loss: 0.316\n",
            "[3, 26800/40625] Loss: 0.342\n",
            "[3, 26900/40625] Loss: 0.299\n",
            "[3, 27000/40625] Loss: 0.292\n",
            "[3, 27100/40625] Loss: 0.380\n",
            "[3, 27200/40625] Loss: 0.392\n",
            "[3, 27300/40625] Loss: 0.417\n",
            "[3, 27400/40625] Loss: 0.285\n",
            "[3, 27500/40625] Loss: 0.392\n",
            "[3, 27600/40625] Loss: 0.298\n",
            "[3, 27700/40625] Loss: 0.293\n",
            "[3, 27800/40625] Loss: 0.329\n",
            "[3, 27900/40625] Loss: 0.365\n",
            "[3, 28000/40625] Loss: 0.373\n",
            "[3, 28100/40625] Loss: 0.328\n",
            "[3, 28200/40625] Loss: 0.302\n",
            "[3, 28300/40625] Loss: 0.348\n",
            "[3, 28400/40625] Loss: 0.323\n",
            "[3, 28500/40625] Loss: 0.310\n",
            "[3, 28600/40625] Loss: 0.302\n",
            "[3, 28700/40625] Loss: 0.249\n",
            "[3, 28800/40625] Loss: 0.380\n",
            "[3, 28900/40625] Loss: 0.355\n",
            "[3, 29000/40625] Loss: 0.252\n",
            "[3, 29100/40625] Loss: 0.216\n",
            "[3, 29200/40625] Loss: 0.385\n",
            "[3, 29300/40625] Loss: 0.319\n",
            "[3, 29400/40625] Loss: 0.451\n",
            "[3, 29500/40625] Loss: 0.279\n",
            "[3, 29600/40625] Loss: 0.223\n",
            "[3, 29700/40625] Loss: 0.301\n",
            "[3, 29800/40625] Loss: 0.384\n",
            "[3, 29900/40625] Loss: 0.260\n",
            "[3, 30000/40625] Loss: 0.287\n",
            "[3, 30100/40625] Loss: 0.276\n",
            "[3, 30200/40625] Loss: 0.318\n",
            "[3, 30300/40625] Loss: 0.280\n",
            "[3, 30400/40625] Loss: 0.245\n",
            "[3, 30500/40625] Loss: 0.257\n",
            "[3, 30600/40625] Loss: 0.250\n",
            "[3, 30700/40625] Loss: 0.254\n",
            "[3, 30800/40625] Loss: 0.441\n",
            "[3, 30900/40625] Loss: 0.463\n",
            "[3, 31000/40625] Loss: 0.312\n",
            "[3, 31100/40625] Loss: 0.297\n",
            "[3, 31200/40625] Loss: 0.310\n",
            "[3, 31300/40625] Loss: 0.386\n",
            "[3, 31400/40625] Loss: 0.274\n",
            "[3, 31500/40625] Loss: 0.338\n",
            "[3, 31600/40625] Loss: 0.310\n",
            "[3, 31700/40625] Loss: 0.356\n",
            "[3, 31800/40625] Loss: 0.335\n",
            "[3, 31900/40625] Loss: 0.438\n",
            "[3, 32000/40625] Loss: 0.413\n",
            "[3, 32100/40625] Loss: 0.243\n",
            "[3, 32200/40625] Loss: 0.347\n",
            "[3, 32300/40625] Loss: 0.291\n",
            "[3, 32400/40625] Loss: 0.331\n",
            "[3, 32500/40625] Loss: 0.399\n",
            "[3, 32600/40625] Loss: 0.268\n",
            "[3, 32700/40625] Loss: 0.420\n",
            "[3, 32800/40625] Loss: 0.215\n",
            "[3, 32900/40625] Loss: 0.416\n",
            "[3, 33000/40625] Loss: 0.378\n",
            "[3, 33100/40625] Loss: 0.343\n",
            "[3, 33200/40625] Loss: 0.327\n",
            "[3, 33300/40625] Loss: 0.436\n",
            "[3, 33400/40625] Loss: 0.384\n",
            "[3, 33500/40625] Loss: 0.275\n",
            "[3, 33600/40625] Loss: 0.353\n",
            "[3, 33700/40625] Loss: 0.355\n",
            "[3, 33800/40625] Loss: 0.345\n",
            "[3, 33900/40625] Loss: 0.268\n",
            "[3, 34000/40625] Loss: 0.450\n",
            "[3, 34100/40625] Loss: 0.322\n",
            "[3, 34200/40625] Loss: 0.221\n",
            "[3, 34300/40625] Loss: 0.329\n",
            "[3, 34400/40625] Loss: 0.396\n",
            "[3, 34500/40625] Loss: 0.354\n",
            "[3, 34600/40625] Loss: 0.233\n",
            "[3, 34700/40625] Loss: 0.284\n",
            "[3, 34800/40625] Loss: 0.255\n",
            "[3, 34900/40625] Loss: 0.337\n",
            "[3, 35000/40625] Loss: 0.385\n",
            "[3, 35100/40625] Loss: 0.280\n",
            "[3, 35200/40625] Loss: 0.364\n",
            "[3, 35300/40625] Loss: 0.317\n",
            "[3, 35400/40625] Loss: 0.404\n",
            "[3, 35500/40625] Loss: 0.274\n",
            "[3, 35600/40625] Loss: 0.283\n",
            "[3, 35700/40625] Loss: 0.316\n",
            "[3, 35800/40625] Loss: 0.381\n",
            "[3, 35900/40625] Loss: 0.302\n",
            "[3, 36000/40625] Loss: 0.347\n",
            "[3, 36100/40625] Loss: 0.262\n",
            "[3, 36200/40625] Loss: 0.273\n",
            "[3, 36300/40625] Loss: 0.388\n",
            "[3, 36400/40625] Loss: 0.340\n",
            "[3, 36500/40625] Loss: 0.273\n",
            "[3, 36600/40625] Loss: 0.252\n",
            "[3, 36700/40625] Loss: 0.355\n",
            "[3, 36800/40625] Loss: 0.238\n",
            "[3, 36900/40625] Loss: 0.363\n",
            "[3, 37000/40625] Loss: 0.254\n",
            "[3, 37100/40625] Loss: 0.428\n",
            "[3, 37200/40625] Loss: 0.357\n",
            "[3, 37300/40625] Loss: 0.356\n",
            "[3, 37400/40625] Loss: 0.362\n",
            "[3, 37500/40625] Loss: 0.286\n",
            "[3, 37600/40625] Loss: 0.383\n",
            "[3, 37700/40625] Loss: 0.328\n",
            "[3, 37800/40625] Loss: 0.285\n",
            "[3, 37900/40625] Loss: 0.212\n",
            "[3, 38000/40625] Loss: 0.273\n",
            "[3, 38100/40625] Loss: 0.363\n",
            "[3, 38200/40625] Loss: 0.349\n",
            "[3, 38300/40625] Loss: 0.305\n",
            "[3, 38400/40625] Loss: 0.329\n",
            "[3, 38500/40625] Loss: 0.277\n",
            "[3, 38600/40625] Loss: 0.365\n",
            "[3, 38700/40625] Loss: 0.330\n",
            "[3, 38800/40625] Loss: 0.359\n",
            "[3, 38900/40625] Loss: 0.249\n",
            "[3, 39000/40625] Loss: 0.265\n",
            "[3, 39100/40625] Loss: 0.376\n",
            "[3, 39200/40625] Loss: 0.454\n",
            "[3, 39300/40625] Loss: 0.398\n",
            "[3, 39400/40625] Loss: 0.338\n",
            "[3, 39500/40625] Loss: 0.406\n",
            "[3, 39600/40625] Loss: 0.266\n",
            "[3, 39700/40625] Loss: 0.326\n",
            "[3, 39800/40625] Loss: 0.289\n",
            "[3, 39900/40625] Loss: 0.335\n",
            "[3, 40000/40625] Loss: 0.341\n",
            "[3, 40100/40625] Loss: 0.432\n",
            "[3, 40200/40625] Loss: 0.427\n",
            "[3, 40300/40625] Loss: 0.461\n",
            "[3, 40400/40625] Loss: 0.432\n",
            "[3, 40500/40625] Loss: 0.299\n",
            "[3, 40600/40625] Loss: 0.342\n",
            "[4,   100/40625] Loss: 0.354\n",
            "[4,   200/40625] Loss: 0.286\n",
            "[4,   300/40625] Loss: 0.369\n",
            "[4,   400/40625] Loss: 0.227\n",
            "[4,   500/40625] Loss: 0.314\n",
            "[4,   600/40625] Loss: 0.359\n",
            "[4,   700/40625] Loss: 0.224\n",
            "[4,   800/40625] Loss: 0.305\n",
            "[4,   900/40625] Loss: 0.479\n",
            "[4,  1000/40625] Loss: 0.329\n",
            "[4,  1100/40625] Loss: 0.262\n",
            "[4,  1200/40625] Loss: 0.367\n",
            "[4,  1300/40625] Loss: 0.264\n",
            "[4,  1400/40625] Loss: 0.278\n",
            "[4,  1500/40625] Loss: 0.354\n",
            "[4,  1600/40625] Loss: 0.315\n",
            "[4,  1700/40625] Loss: 0.223\n",
            "[4,  1800/40625] Loss: 0.373\n",
            "[4,  1900/40625] Loss: 0.241\n",
            "[4,  2000/40625] Loss: 0.218\n",
            "[4,  2100/40625] Loss: 0.308\n",
            "[4,  2200/40625] Loss: 0.265\n",
            "[4,  2300/40625] Loss: 0.417\n",
            "[4,  2400/40625] Loss: 0.307\n",
            "[4,  2500/40625] Loss: 0.392\n",
            "[4,  2600/40625] Loss: 0.296\n",
            "[4,  2700/40625] Loss: 0.254\n",
            "[4,  2800/40625] Loss: 0.257\n",
            "[4,  2900/40625] Loss: 0.315\n",
            "[4,  3000/40625] Loss: 0.231\n",
            "[4,  3100/40625] Loss: 0.499\n",
            "[4,  3200/40625] Loss: 0.292\n",
            "[4,  3300/40625] Loss: 0.312\n",
            "[4,  3400/40625] Loss: 0.339\n",
            "[4,  3500/40625] Loss: 0.363\n",
            "[4,  3600/40625] Loss: 0.318\n",
            "[4,  3700/40625] Loss: 0.306\n",
            "[4,  3800/40625] Loss: 0.326\n",
            "[4,  3900/40625] Loss: 0.316\n",
            "[4,  4000/40625] Loss: 0.330\n",
            "[4,  4100/40625] Loss: 0.342\n",
            "[4,  4200/40625] Loss: 0.369\n",
            "[4,  4300/40625] Loss: 0.261\n",
            "[4,  4400/40625] Loss: 0.281\n",
            "[4,  4500/40625] Loss: 0.330\n",
            "[4,  4600/40625] Loss: 0.354\n",
            "[4,  4700/40625] Loss: 0.326\n",
            "[4,  4800/40625] Loss: 0.314\n",
            "[4,  4900/40625] Loss: 0.331\n",
            "[4,  5000/40625] Loss: 0.281\n",
            "[4,  5100/40625] Loss: 0.272\n",
            "[4,  5200/40625] Loss: 0.348\n",
            "[4,  5300/40625] Loss: 0.358\n",
            "[4,  5400/40625] Loss: 0.381\n",
            "[4,  5500/40625] Loss: 0.282\n",
            "[4,  5600/40625] Loss: 0.269\n",
            "[4,  5700/40625] Loss: 0.439\n",
            "[4,  5800/40625] Loss: 0.356\n",
            "[4,  5900/40625] Loss: 0.250\n",
            "[4,  6000/40625] Loss: 0.332\n",
            "[4,  6100/40625] Loss: 0.378\n",
            "[4,  6200/40625] Loss: 0.200\n",
            "[4,  6300/40625] Loss: 0.440\n",
            "[4,  6400/40625] Loss: 0.319\n",
            "[4,  6500/40625] Loss: 0.232\n",
            "[4,  6600/40625] Loss: 0.207\n",
            "[4,  6700/40625] Loss: 0.205\n",
            "[4,  6800/40625] Loss: 0.317\n",
            "[4,  6900/40625] Loss: 0.268\n",
            "[4,  7000/40625] Loss: 0.290\n",
            "[4,  7100/40625] Loss: 0.362\n",
            "[4,  7200/40625] Loss: 0.346\n",
            "[4,  7300/40625] Loss: 0.326\n",
            "[4,  7400/40625] Loss: 0.293\n",
            "[4,  7500/40625] Loss: 0.307\n",
            "[4,  7600/40625] Loss: 0.377\n",
            "[4,  7700/40625] Loss: 0.305\n",
            "[4,  7800/40625] Loss: 0.390\n",
            "[4,  7900/40625] Loss: 0.330\n",
            "[4,  8000/40625] Loss: 0.351\n",
            "[4,  8100/40625] Loss: 0.302\n",
            "[4,  8200/40625] Loss: 0.303\n",
            "[4,  8300/40625] Loss: 0.360\n",
            "[4,  8400/40625] Loss: 0.282\n",
            "[4,  8500/40625] Loss: 0.323\n",
            "[4,  8600/40625] Loss: 0.416\n",
            "[4,  8700/40625] Loss: 0.393\n",
            "[4,  8800/40625] Loss: 0.283\n",
            "[4,  8900/40625] Loss: 0.287\n",
            "[4,  9000/40625] Loss: 0.303\n",
            "[4,  9100/40625] Loss: 0.265\n",
            "[4,  9200/40625] Loss: 0.253\n",
            "[4,  9300/40625] Loss: 0.340\n",
            "[4,  9400/40625] Loss: 0.237\n",
            "[4,  9500/40625] Loss: 0.297\n",
            "[4,  9600/40625] Loss: 0.256\n",
            "[4,  9700/40625] Loss: 0.288\n",
            "[4,  9800/40625] Loss: 0.264\n",
            "[4,  9900/40625] Loss: 0.408\n",
            "[4, 10000/40625] Loss: 0.334\n",
            "[4, 10100/40625] Loss: 0.319\n",
            "[4, 10200/40625] Loss: 0.254\n",
            "[4, 10300/40625] Loss: 0.312\n",
            "[4, 10400/40625] Loss: 0.245\n",
            "[4, 10500/40625] Loss: 0.341\n",
            "[4, 10600/40625] Loss: 0.360\n",
            "[4, 10700/40625] Loss: 0.408\n",
            "[4, 10800/40625] Loss: 0.297\n",
            "[4, 10900/40625] Loss: 0.388\n",
            "[4, 11000/40625] Loss: 0.324\n",
            "[4, 11100/40625] Loss: 0.381\n",
            "[4, 11200/40625] Loss: 0.352\n",
            "[4, 11300/40625] Loss: 0.287\n",
            "[4, 11400/40625] Loss: 0.380\n",
            "[4, 11500/40625] Loss: 0.392\n",
            "[4, 11600/40625] Loss: 0.268\n",
            "[4, 11700/40625] Loss: 0.457\n",
            "[4, 11800/40625] Loss: 0.318\n",
            "[4, 11900/40625] Loss: 0.263\n",
            "[4, 12000/40625] Loss: 0.419\n",
            "[4, 12100/40625] Loss: 0.381\n",
            "[4, 12200/40625] Loss: 0.414\n",
            "[4, 12300/40625] Loss: 0.280\n",
            "[4, 12400/40625] Loss: 0.270\n",
            "[4, 12500/40625] Loss: 0.269\n",
            "[4, 12600/40625] Loss: 0.349\n",
            "[4, 12700/40625] Loss: 0.227\n",
            "[4, 12800/40625] Loss: 0.258\n",
            "[4, 12900/40625] Loss: 0.406\n",
            "[4, 13000/40625] Loss: 0.341\n",
            "[4, 13100/40625] Loss: 0.300\n",
            "[4, 13200/40625] Loss: 0.347\n",
            "[4, 13300/40625] Loss: 0.237\n",
            "[4, 13400/40625] Loss: 0.348\n",
            "[4, 13500/40625] Loss: 0.393\n",
            "[4, 13600/40625] Loss: 0.340\n",
            "[4, 13700/40625] Loss: 0.325\n",
            "[4, 13800/40625] Loss: 0.332\n",
            "[4, 13900/40625] Loss: 0.258\n",
            "[4, 14000/40625] Loss: 0.222\n",
            "[4, 14100/40625] Loss: 0.331\n",
            "[4, 14200/40625] Loss: 0.360\n",
            "[4, 14300/40625] Loss: 0.278\n",
            "[4, 14400/40625] Loss: 0.262\n",
            "[4, 14500/40625] Loss: 0.321\n",
            "[4, 14600/40625] Loss: 0.225\n",
            "[4, 14700/40625] Loss: 0.358\n",
            "[4, 14800/40625] Loss: 0.237\n",
            "[4, 14900/40625] Loss: 0.295\n",
            "[4, 15000/40625] Loss: 0.345\n",
            "[4, 15100/40625] Loss: 0.213\n",
            "[4, 15200/40625] Loss: 0.304\n",
            "[4, 15300/40625] Loss: 0.314\n",
            "[4, 15400/40625] Loss: 0.251\n",
            "[4, 15500/40625] Loss: 0.273\n",
            "[4, 15600/40625] Loss: 0.301\n",
            "[4, 15700/40625] Loss: 0.352\n",
            "[4, 15800/40625] Loss: 0.265\n",
            "[4, 15900/40625] Loss: 0.325\n",
            "[4, 16000/40625] Loss: 0.317\n",
            "[4, 16100/40625] Loss: 0.252\n",
            "[4, 16200/40625] Loss: 0.140\n",
            "[4, 16300/40625] Loss: 0.310\n",
            "[4, 16400/40625] Loss: 0.216\n",
            "[4, 16500/40625] Loss: 0.389\n",
            "[4, 16600/40625] Loss: 0.199\n",
            "[4, 16700/40625] Loss: 0.227\n",
            "[4, 16800/40625] Loss: 0.299\n",
            "[4, 16900/40625] Loss: 0.280\n",
            "[4, 17000/40625] Loss: 0.285\n",
            "[4, 17100/40625] Loss: 0.373\n",
            "[4, 17200/40625] Loss: 0.306\n",
            "[4, 17300/40625] Loss: 0.256\n",
            "[4, 17400/40625] Loss: 0.367\n",
            "[4, 17500/40625] Loss: 0.378\n",
            "[4, 17600/40625] Loss: 0.357\n",
            "[4, 17700/40625] Loss: 0.318\n",
            "[4, 17800/40625] Loss: 0.253\n",
            "[4, 17900/40625] Loss: 0.440\n",
            "[4, 18000/40625] Loss: 0.284\n",
            "[4, 18100/40625] Loss: 0.261\n",
            "[4, 18200/40625] Loss: 0.316\n",
            "[4, 18300/40625] Loss: 0.295\n",
            "[4, 18400/40625] Loss: 0.242\n",
            "[4, 18500/40625] Loss: 0.325\n",
            "[4, 18600/40625] Loss: 0.294\n",
            "[4, 18700/40625] Loss: 0.285\n",
            "[4, 18800/40625] Loss: 0.370\n",
            "[4, 18900/40625] Loss: 0.336\n",
            "[4, 19000/40625] Loss: 0.310\n",
            "[4, 19100/40625] Loss: 0.346\n",
            "[4, 19200/40625] Loss: 0.332\n",
            "[4, 19300/40625] Loss: 0.240\n",
            "[4, 19400/40625] Loss: 0.277\n",
            "[4, 19500/40625] Loss: 0.351\n",
            "[4, 19600/40625] Loss: 0.312\n",
            "[4, 19700/40625] Loss: 0.323\n",
            "[4, 19800/40625] Loss: 0.305\n",
            "[4, 19900/40625] Loss: 0.289\n",
            "[4, 20000/40625] Loss: 0.455\n",
            "[4, 20100/40625] Loss: 0.218\n",
            "[4, 20200/40625] Loss: 0.270\n",
            "[4, 20300/40625] Loss: 0.334\n",
            "[4, 20400/40625] Loss: 0.336\n",
            "[4, 20500/40625] Loss: 0.260\n",
            "[4, 20600/40625] Loss: 0.299\n",
            "[4, 20700/40625] Loss: 0.387\n",
            "[4, 20800/40625] Loss: 0.432\n",
            "[4, 20900/40625] Loss: 0.464\n",
            "[4, 21000/40625] Loss: 0.343\n",
            "[4, 21100/40625] Loss: 0.391\n",
            "[4, 21200/40625] Loss: 0.210\n",
            "[4, 21300/40625] Loss: 0.307\n",
            "[4, 21400/40625] Loss: 0.299\n",
            "[4, 21500/40625] Loss: 0.433\n",
            "[4, 21600/40625] Loss: 0.344\n",
            "[4, 21700/40625] Loss: 0.408\n",
            "[4, 21800/40625] Loss: 0.276\n",
            "[4, 21900/40625] Loss: 0.343\n",
            "[4, 22000/40625] Loss: 0.414\n",
            "[4, 22100/40625] Loss: 0.282\n",
            "[4, 22200/40625] Loss: 0.289\n",
            "[4, 22300/40625] Loss: 0.353\n",
            "[4, 22400/40625] Loss: 0.293\n",
            "[4, 22500/40625] Loss: 0.300\n",
            "[4, 22600/40625] Loss: 0.248\n",
            "[4, 22700/40625] Loss: 0.368\n",
            "[4, 22800/40625] Loss: 0.378\n",
            "[4, 22900/40625] Loss: 0.296\n",
            "[4, 23000/40625] Loss: 0.254\n",
            "[4, 23100/40625] Loss: 0.230\n",
            "[4, 23200/40625] Loss: 0.286\n",
            "[4, 23300/40625] Loss: 0.207\n",
            "[4, 23400/40625] Loss: 0.329\n",
            "[4, 23500/40625] Loss: 0.390\n",
            "[4, 23600/40625] Loss: 0.317\n",
            "[4, 23700/40625] Loss: 0.311\n",
            "[4, 23800/40625] Loss: 0.253\n",
            "[4, 23900/40625] Loss: 0.234\n",
            "[4, 24000/40625] Loss: 0.254\n",
            "[4, 24100/40625] Loss: 0.248\n",
            "[4, 24200/40625] Loss: 0.191\n",
            "[4, 24300/40625] Loss: 0.262\n",
            "[4, 24400/40625] Loss: 0.298\n",
            "[4, 24500/40625] Loss: 0.452\n",
            "[4, 24600/40625] Loss: 0.299\n",
            "[4, 24700/40625] Loss: 0.275\n",
            "[4, 24800/40625] Loss: 0.295\n",
            "[4, 24900/40625] Loss: 0.269\n",
            "[4, 25000/40625] Loss: 0.349\n",
            "[4, 25100/40625] Loss: 0.257\n",
            "[4, 25200/40625] Loss: 0.324\n",
            "[4, 25300/40625] Loss: 0.306\n",
            "[4, 25400/40625] Loss: 0.252\n",
            "[4, 25500/40625] Loss: 0.337\n",
            "[4, 25600/40625] Loss: 0.369\n",
            "[4, 25700/40625] Loss: 0.288\n",
            "[4, 25800/40625] Loss: 0.390\n",
            "[4, 25900/40625] Loss: 0.216\n",
            "[4, 26000/40625] Loss: 0.316\n",
            "[4, 26100/40625] Loss: 0.297\n",
            "[4, 26200/40625] Loss: 0.300\n",
            "[4, 26300/40625] Loss: 0.216\n",
            "[4, 26400/40625] Loss: 0.237\n",
            "[4, 26500/40625] Loss: 0.347\n",
            "[4, 26600/40625] Loss: 0.436\n",
            "[4, 26700/40625] Loss: 0.290\n",
            "[4, 26800/40625] Loss: 0.303\n",
            "[4, 26900/40625] Loss: 0.282\n",
            "[4, 27000/40625] Loss: 0.293\n",
            "[4, 27100/40625] Loss: 0.346\n",
            "[4, 27200/40625] Loss: 0.321\n",
            "[4, 27300/40625] Loss: 0.399\n",
            "[4, 27400/40625] Loss: 0.240\n",
            "[4, 27500/40625] Loss: 0.327\n",
            "[4, 27600/40625] Loss: 0.309\n",
            "[4, 27700/40625] Loss: 0.252\n",
            "[4, 27800/40625] Loss: 0.319\n",
            "[4, 27900/40625] Loss: 0.277\n",
            "[4, 28000/40625] Loss: 0.395\n",
            "[4, 28100/40625] Loss: 0.276\n",
            "[4, 28200/40625] Loss: 0.268\n",
            "[4, 28300/40625] Loss: 0.335\n",
            "[4, 28400/40625] Loss: 0.273\n",
            "[4, 28500/40625] Loss: 0.343\n",
            "[4, 28600/40625] Loss: 0.388\n",
            "[4, 28700/40625] Loss: 0.297\n",
            "[4, 28800/40625] Loss: 0.400\n",
            "[4, 28900/40625] Loss: 0.298\n",
            "[4, 29000/40625] Loss: 0.268\n",
            "[4, 29100/40625] Loss: 0.205\n",
            "[4, 29200/40625] Loss: 0.415\n",
            "[4, 29300/40625] Loss: 0.318\n",
            "[4, 29400/40625] Loss: 0.373\n",
            "[4, 29500/40625] Loss: 0.290\n",
            "[4, 29600/40625] Loss: 0.202\n",
            "[4, 29700/40625] Loss: 0.292\n",
            "[4, 29800/40625] Loss: 0.361\n",
            "[4, 29900/40625] Loss: 0.219\n",
            "[4, 30000/40625] Loss: 0.259\n",
            "[4, 30100/40625] Loss: 0.304\n",
            "[4, 30200/40625] Loss: 0.318\n",
            "[4, 30300/40625] Loss: 0.319\n",
            "[4, 30400/40625] Loss: 0.243\n",
            "[4, 30500/40625] Loss: 0.302\n",
            "[4, 30600/40625] Loss: 0.199\n",
            "[4, 30700/40625] Loss: 0.281\n",
            "[4, 30800/40625] Loss: 0.393\n",
            "[4, 30900/40625] Loss: 0.319\n",
            "[4, 31000/40625] Loss: 0.314\n",
            "[4, 31100/40625] Loss: 0.191\n",
            "[4, 31200/40625] Loss: 0.243\n",
            "[4, 31300/40625] Loss: 0.291\n",
            "[4, 31400/40625] Loss: 0.225\n",
            "[4, 31500/40625] Loss: 0.278\n",
            "[4, 31600/40625] Loss: 0.293\n",
            "[4, 31700/40625] Loss: 0.292\n",
            "[4, 31800/40625] Loss: 0.307\n",
            "[4, 31900/40625] Loss: 0.388\n",
            "[4, 32000/40625] Loss: 0.390\n",
            "[4, 32100/40625] Loss: 0.293\n",
            "[4, 32200/40625] Loss: 0.333\n",
            "[4, 32300/40625] Loss: 0.299\n",
            "[4, 32400/40625] Loss: 0.323\n",
            "[4, 32500/40625] Loss: 0.350\n",
            "[4, 32600/40625] Loss: 0.311\n",
            "[4, 32700/40625] Loss: 0.315\n",
            "[4, 32800/40625] Loss: 0.202\n",
            "[4, 32900/40625] Loss: 0.359\n",
            "[4, 33000/40625] Loss: 0.353\n",
            "[4, 33100/40625] Loss: 0.362\n",
            "[4, 33200/40625] Loss: 0.252\n",
            "[4, 33300/40625] Loss: 0.310\n",
            "[4, 33400/40625] Loss: 0.306\n",
            "[4, 33500/40625] Loss: 0.380\n",
            "[4, 33600/40625] Loss: 0.369\n",
            "[4, 33700/40625] Loss: 0.401\n",
            "[4, 33800/40625] Loss: 0.296\n",
            "[4, 33900/40625] Loss: 0.273\n",
            "[4, 34000/40625] Loss: 0.398\n",
            "[4, 34100/40625] Loss: 0.293\n",
            "[4, 34200/40625] Loss: 0.241\n",
            "[4, 34300/40625] Loss: 0.303\n",
            "[4, 34400/40625] Loss: 0.477\n",
            "[4, 34500/40625] Loss: 0.341\n",
            "[4, 34600/40625] Loss: 0.232\n",
            "[4, 34700/40625] Loss: 0.227\n",
            "[4, 34800/40625] Loss: 0.213\n",
            "[4, 34900/40625] Loss: 0.308\n",
            "[4, 35000/40625] Loss: 0.369\n",
            "[4, 35100/40625] Loss: 0.215\n",
            "[4, 35200/40625] Loss: 0.300\n",
            "[4, 35300/40625] Loss: 0.299\n",
            "[4, 35400/40625] Loss: 0.307\n",
            "[4, 35500/40625] Loss: 0.316\n",
            "[4, 35600/40625] Loss: 0.280\n",
            "[4, 35700/40625] Loss: 0.286\n",
            "[4, 35800/40625] Loss: 0.373\n",
            "[4, 35900/40625] Loss: 0.242\n",
            "[4, 36000/40625] Loss: 0.246\n",
            "[4, 36100/40625] Loss: 0.270\n",
            "[4, 36200/40625] Loss: 0.250\n",
            "[4, 36300/40625] Loss: 0.371\n",
            "[4, 36400/40625] Loss: 0.320\n",
            "[4, 36500/40625] Loss: 0.282\n",
            "[4, 36600/40625] Loss: 0.190\n",
            "[4, 36700/40625] Loss: 0.270\n",
            "[4, 36800/40625] Loss: 0.228\n",
            "[4, 36900/40625] Loss: 0.301\n",
            "[4, 37000/40625] Loss: 0.185\n",
            "[4, 37100/40625] Loss: 0.391\n",
            "[4, 37200/40625] Loss: 0.317\n",
            "[4, 37300/40625] Loss: 0.313\n",
            "[4, 37400/40625] Loss: 0.297\n",
            "[4, 37500/40625] Loss: 0.207\n",
            "[4, 37600/40625] Loss: 0.385\n",
            "[4, 37700/40625] Loss: 0.302\n",
            "[4, 37800/40625] Loss: 0.283\n",
            "[4, 37900/40625] Loss: 0.211\n",
            "[4, 38000/40625] Loss: 0.285\n",
            "[4, 38100/40625] Loss: 0.349\n",
            "[4, 38200/40625] Loss: 0.304\n",
            "[4, 38300/40625] Loss: 0.350\n",
            "[4, 38400/40625] Loss: 0.321\n",
            "[4, 38500/40625] Loss: 0.371\n",
            "[4, 38600/40625] Loss: 0.271\n",
            "[4, 38700/40625] Loss: 0.352\n",
            "[4, 38800/40625] Loss: 0.340\n",
            "[4, 38900/40625] Loss: 0.219\n",
            "[4, 39000/40625] Loss: 0.264\n",
            "[4, 39100/40625] Loss: 0.344\n",
            "[4, 39200/40625] Loss: 0.337\n",
            "[4, 39300/40625] Loss: 0.354\n",
            "[4, 39400/40625] Loss: 0.278\n",
            "[4, 39500/40625] Loss: 0.404\n",
            "[4, 39600/40625] Loss: 0.229\n",
            "[4, 39700/40625] Loss: 0.263\n",
            "[4, 39800/40625] Loss: 0.319\n",
            "[4, 39900/40625] Loss: 0.355\n",
            "[4, 40000/40625] Loss: 0.324\n",
            "[4, 40100/40625] Loss: 0.402\n",
            "[4, 40200/40625] Loss: 0.449\n",
            "[4, 40300/40625] Loss: 0.352\n",
            "[4, 40400/40625] Loss: 0.293\n",
            "[4, 40500/40625] Loss: 0.311\n",
            "[4, 40600/40625] Loss: 0.239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "DEOHR_MbR1Xl",
        "outputId": "93949991-dfec-401f-a535-dcbc2e2e8902"
      },
      "source": [
        "##loss graph\r\n",
        "plt.figure(figsize = (15,7))\r\n",
        "plt.subplot(1, 2, 2)\r\n",
        "plt.plot(epoch_array, train_loss_array, 'b', label='Training Loss')\r\n",
        "plt.title('Train Loss Vs. Epochs', fontsize = 20, weight='bold')\r\n",
        "plt.xlabel('epochs')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.legend(loc = 'upper right')\r\n",
        "plt.grid()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5OyAPyrN19u"
      },
      "source": [
        "# Task 3\n",
        "Write some code for evaluating your model on the dev set. Since the data is almost balanced (there are 52 positives in the dev set), let's print accuracy (i.e., the number of correctly classified instances)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1FvyRM8O7Re",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f698059-f477-4fbb-9f31-c0209ccdcd39"
      },
      "source": [
        "# Your evalutation code goes here\r\n",
        "correct = 0\r\n",
        "total = 0\r\n",
        "with torch.no_grad():\r\n",
        "  for sequence in dev:\r\n",
        "    inputs,label=sequence\r\n",
        "    inputs = torch.LongTensor(inputs).cuda()  \r\n",
        "    label = torch.LongTensor([label]).cuda()\r\n",
        "    output = model(inputs).cuda()\r\n",
        "    _, predicted = torch.max(output, 1)\r\n",
        "    total += 1\r\n",
        "    correct += (predicted == label).sum().item()\r\n",
        "\r\n",
        "\r\n",
        "acc = 100.0 * correct / total\r\n",
        "print(f'Accuracy: {acc} %')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 85.71428571428571 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}